{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "962925dc",
   "metadata": {},
   "source": [
    "# VCI Bayes Explore"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0158c2ba",
   "metadata": {},
   "source": [
    "## 1. Environment Setup\n",
    "\n",
    "Import dependencies, configure plotting defaults, and align pyAgrum's visual output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6297e646",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Core Python utilities\n",
    "import base64\n",
    "import io\n",
    "import os\n",
    "import random\n",
    "from itertools import product\n",
    "from pathlib import Path\n",
    "\n",
    "# Numerical and data processing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.stats import chi2_contingency\n",
    "\n",
    "# Visualisation helpers\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.font_manager as fm\n",
    "from IPython.display import display\n",
    "from ipywidgets import interact, fixed\n",
    "\n",
    "# Bayesian network tooling (pyAgrum ecosystem)\n",
    "import pyagrum as gum\n",
    "import pyagrum.lib.explain as explain\n",
    "import pyagrum.lib.explain as expl\n",
    "import pyagrum.lib.notebook as gnb\n",
    "import pyagrum.lib.utils as gutils\n",
    "import pyagrum.lib.bn2graph as gumb2g\n",
    "import pyagrum.lib.bn_vs_bn as bnvsbn\n",
    "import pyagrum.lib.bn_vs_bn as gcm\n",
    "from pyagrum import BNLearner\n",
    "from pyagrum.lib.bn2roc import showROC\n",
    "from pyagrum.lib.discreteTypeProcessor import DiscreteTypeProcessor\n",
    "import pyagrum.skbn as skbn\n",
    "from pyagrum.skbn import BNClassifier\n",
    "\n",
    "# Scikit-learn helpers\n",
    "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import warnings\n",
    "\n",
    "# Matplotlib defaults for manuscript-ready figures\n",
    "warnings.filterwarnings('ignore', category=UserWarning)\n",
    "\n",
    "mpl.rcParams['font.family'] = 'Helvetica Neue'\n",
    "mpl.rcParams['font.size'] = 14\n",
    "mpl.rcParams['font.weight'] = 'regular'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility: format dictionaries for side-by-side HTML display in the notebook\n",
    "def dict2html(di1, di2=None):\n",
    "  res = \"<br/>\".join([f\"<b>{k:15}</b>:{v}\" for k, v in di1.items()])\n",
    "  if di2 is not None:\n",
    "    res += \"<br/><br/>\"\n",
    "    res += \"<br/>\".join([f\"<b>{k:15}</b>:{v}\" for k, v in di2.items()])\n",
    "  return res\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure pyAgrum renders graphs with consistent layouts across reruns\n",
    "gum.config[\"notebook\", \"graph_layout\"] = \"dot\"\n",
    "gum.config[\"notebook\", \"graph_rankdir\"] = \"TB\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading and Preparation\n",
    "\n",
    "Set `PROJECT_ROOT` so that `PROJECT_ROOT / data` holds the parquet exports required for the analysis, or run `projects/HBC/preprocess_data.py` after configuring `config/data_paths.yml` to recreate them.\n",
    "\n",
    "This notebook expects three inputs:\n",
    "- `df.parquet`: raw cohort data without imputation (used for comparison plots).\n",
    "- `df_imp.parquet`: fully imputed dataset feeding the Bayesian network learner.\n",
    "- `bn_vars.parquet`: metadata table with at least `LAYER` and `VARIABLE NAME` columns describing the expert-assigned layer of each variable.\n",
    "\n",
    "Update the configuration cell below if your repository layout differs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc30a5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd()\n",
    "\n",
    "\n",
    "def _get_path_override(key: str, project_root: Path) -> Path | None:\n",
    "    \"\"\"Return path override from env var or .env without hard-coding paths.\"\"\"\n",
    "    env_value = os.environ.get(key)\n",
    "    if env_value:\n",
    "        return Path(env_value).expanduser()\n",
    "\n",
    "    env_path = project_root / '.env'\n",
    "    if not env_path.exists():\n",
    "        return None\n",
    "\n",
    "    for raw_line in env_path.read_text().splitlines():\n",
    "        line = raw_line.strip()\n",
    "        if not line or line.startswith('#') or '=' not in line:\n",
    "            continue\n",
    "        name, value = line.split('=', 1)\n",
    "        if name.strip() == key:\n",
    "            cleaned = value.strip()\n",
    "            if cleaned[:1] in {'\"', \"'\"} and cleaned[-1:] == cleaned[:1]:\n",
    "                cleaned = cleaned[1:-1]\n",
    "            if cleaned:\n",
    "                return Path(cleaned).expanduser()\n",
    "    return None\n",
    "\n",
    "\n",
    "candidate_dirs = []\n",
    "\n",
    "override_path = _get_path_override('BAYESIAN_DATA_DIR', PROJECT_ROOT)\n",
    "if override_path:\n",
    "    if not override_path.exists():\n",
    "        raise FileNotFoundError(\n",
    "            f\"BAYESIAN_DATA_DIR={str(override_path)!r} does not exist. \"\n",
    "            \"Update the override value or remove it.\"\n",
    "        )\n",
    "    candidate_dirs.append(override_path)\n",
    "\n",
    "candidate_dirs += [\n",
    "    PROJECT_ROOT / 'src' / 'out',\n",
    "    PROJECT_ROOT / 'out',\n",
    "    PROJECT_ROOT / 'data',\n",
    "]\n",
    "\n",
    "for candidate in candidate_dirs:\n",
    "    if candidate.exists():\n",
    "        DATA_DIR = candidate\n",
    "        break\n",
    "else:\n",
    "    DATA_DIR = PROJECT_ROOT / 'data'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load harmonised datasets used throughout the analysis\n",
    "df = pd.read_parquet(DATA_DIR / 'df.parquet', engine='pyarrow')\n",
    "df_imp = pd.read_parquet(DATA_DIR / 'df_imp.parquet', engine='pyarrow')\n",
    "bn_vars = pd.read_parquet(DATA_DIR / 'bn_vars.parquet', engine='pyarrow')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54e8fe7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Harmonise categorical entries before discretisation\n",
    "for frame in (df, df_imp):\n",
    "    # Replace blank or missing stroke history entries with explicit 'No'\n",
    "    if 'STROKE HISTORY' in frame.columns:\n",
    "        frame['STROKE HISTORY'] = frame['STROKE HISTORY'].replace({'': 'No', ' ': 'No'}).fillna('No')\n",
    "    # Legacy CVA column cleaning (if present)\n",
    "    if 'CVA' in frame.columns:\n",
    "        frame['CVA'] = frame['CVA'].replace({'': 'Nee', ' ': 'Nee'}).fillna('Nee')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9809fa8c",
   "metadata": {},
   "source": [
    "### 2.2 Metadata sanity check\n",
    "The `bn_vars` table should map every variable used in the network to its corresponding layer.\n",
    "If you add new variables, ensure the parquet file lists them with the desired `LAYER` label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "045cd6ba",
   "metadata": {},
   "source": [
    "Example `bn_vars.parquet` structure (first columns):\n",
    "\n",
    "| LAYER | VARIABLE NAME | DESCRIPTION |\n",
    "| --- | --- | --- |\n",
    "| L0 \u2013 Unmodifiable demographics | AGE | Baseline age in years |\n",
    "| L0 \u2013 Unmodifiable demographics | SEX | Biological sex at baseline |\n",
    "| L2 \u2013 Cardiovascular risk factors | VASCULAR RISK SCORE | SCORE2 cardiovascular risk estimate |\n",
    "| L4 \u2013 Potential disease process markers | PTAU181 | Plasma pTau181 concentration category |\n",
    "| L6 \u2013 Current and previous cardiovascular diagnoses / Vascular interventions | PATIENT GROUP | Diagnostic cohort label |\n",
    "| L8 \u2013 Outcomes | OUTCOME_MACE | Binary indicator of follow-up MACE |\n",
    "\n",
    "Add extra columns (e.g. `DISPLAY NAME`, `NOTES`) as needed\u2014the loader only requires `LAYER` and `VARIABLE NAME`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f95d8c5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ensure DROPOUT REASON is mapped to the dropout layer\n",
    "if 'bn_vars' in globals():\n",
    "    if 'DROPOUT REASON' not in bn_vars['VARIABLE NAME'].values:\n",
    "        bn_vars = pd.concat([bn_vars, pd.DataFrame([{'LAYER': 'L9 \u2013 Dropout', 'VARIABLE NAME': 'DROPOUT REASON'}])], ignore_index=True)\n",
    "    else:\n",
    "        bn_vars.loc[bn_vars['VARIABLE NAME'] == 'DROPOUT REASON', 'LAYER'] = 'L9 \u2013 Dropout'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b06d24f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview the first few rows once bn_vars.parquet is loaded\n",
    "bn_vars.sort_values(['LAYER', 'VARIABLE NAME'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7fca9ef1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Validate the metadata layout before building the network\n",
    "expected_columns = {'LAYER', 'VARIABLE NAME'}\n",
    "missing_cols = expected_columns - set(bn_vars.columns)\n",
    "if missing_cols:\n",
    "    raise ValueError(f\"bn_vars is missing required columns: {missing_cols}\")\n",
    "\n",
    "print('Layers discovered in bn_vars:', sorted(bn_vars['LAYER'].unique()))\n",
    "display(bn_vars.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Variable Discretisation\n",
    "\n",
    "Configure the binning strategy feeding each learner and inspect the resulting template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure discretisation defaults for mixed-type clinical variables\n",
    "type_processor = DiscreteTypeProcessor(\n",
    "  defaultDiscretizationMethod=\"quantile\", defaultNumberOfBins=4, discretizationThreshold=10\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the discretised template to document the bins used by the learner\n",
    "# creating a template explaining the variables proposed by the type_processor. This variables will be used by the learner\n",
    "template = type_processor.discretizedTemplate(df_imp)\n",
    "for i, n in template:\n",
    "  print(f\"{n:7} : {template.variable(i)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspect the discretised template to document the bins used by the learner\n",
    "template = type_processor.discretizedTemplate(df_imp)\n",
    "\n",
    "def format_label(label):\n",
    "    parts = label.strip('([)').split(';')\n",
    "    lower = float(parts[0])\n",
    "    upper = float(parts[1].replace('[', '').replace(')', ''))\n",
    "\n",
    "    lower_rounded = round(lower, 1)\n",
    "    upper_rounded = round(upper, 1)\n",
    "\n",
    "    return f\"({lower_rounded};{upper_rounded}[\"\n",
    "\n",
    "for i, n in template:\n",
    "    var = template.variable(i)\n",
    "    labels = var.labels()\n",
    "\n",
    "    # Check if the labels look like intervals\n",
    "    if all((\";\" in label and \"[\" in label) for label in labels):\n",
    "        formatted_labels = [format_label(label) for label in labels]\n",
    "        print(f\"{n:7} : {formatted_labels}\")\n",
    "    else:\n",
    "        print(f\"{n:7} : {labels}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Optional: uncomment to audit discretisation results\n",
    "#auditDict = type_processor.audit(df_imp)\n",
    "\n",
    "#print()\n",
    "#print(\"** audit **\")\n",
    "#for var in auditDict:\n",
    "#  print(f\"- {var} : \")\n",
    "#  for k, v in auditDict[var].items():\n",
    "#    print(f\"    + {k} : {v}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Knowledge-Guided Network Definition\n",
    "\n",
    "Translate the expert-defined layers into a reusable Bayesian network template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Map variables onto expert-defined layers for downstream constraints\n",
    "layer_map = (\n",
    "    bn_vars.groupby('LAYER')['VARIABLE NAME']\n",
    "    .apply(list)\n",
    "    .to_dict()\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiate the Bayesian network using the discretised template as blueprint\n",
    "bn = gum.BayesNet(\"LayeredBN\")\n",
    "node_ids = {}\n",
    "\n",
    "for i in range(template.size()):\n",
    "    var = template.variable(i)\n",
    "    name = var.name()\n",
    "    node_ids[name] = bn.add(var)  # only add once, from template\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preserve the expert-defined order of layers when applying constraints\n",
    "# Make sure keys are in the correct order\n",
    "layer_keys = list(layer_map.keys())  # use list, not sorted, to preserve domain-defined order"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Constrained Structure Learning\n",
    "\n",
    "Estimate networks that respect the layer order while testing different outcome configurations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Utility: learn a Bayesian network with optional guided structure constraints\n",
    "import random\n",
    "from itertools import product\n",
    "\n",
    "\n",
    "def configure_guided_structure(learner, layer_map_filtered, variables_to_keep, outcomes, enforce=True):\n",
    "    \"\"\"Apply dropout-aware arc constraints when enforce=True.\"\"\"\n",
    "    layer_keys = list(layer_map_filtered.keys())\n",
    "    dropout_layer_keys = [k for k in layer_keys if \"Dropout\" in k]\n",
    "    dropout_vars = [var for key in dropout_layer_keys for var in layer_map_filtered[key]]\n",
    "    true_outcomes = [var for var in outcomes if \"DROPOUT\" not in var.upper()]\n",
    "\n",
    "    if not enforce:\n",
    "        return dropout_vars, true_outcomes\n",
    "\n",
    "    allowed_arcs = []\n",
    "    for i, from_key in enumerate(layer_keys):\n",
    "        from_vars = layer_map_filtered[from_key]\n",
    "        for to_key in layer_keys[i:]:\n",
    "            to_vars = layer_map_filtered[to_key]\n",
    "            for parent, child in product(from_vars, to_vars):\n",
    "                if child in dropout_vars:\n",
    "                    if parent in outcomes:\n",
    "                        allowed_arcs.append((parent, child))\n",
    "                    continue\n",
    "                if parent in variables_to_keep and child in variables_to_keep:\n",
    "                    allowed_arcs.append((parent, child))\n",
    "\n",
    "    for parent in variables_to_keep:\n",
    "        for child in variables_to_keep:\n",
    "            if parent != child and (parent, child) not in allowed_arcs:\n",
    "                learner.addForbiddenArc(parent, child)\n",
    "\n",
    "    for parent in true_outcomes:\n",
    "        for child in true_outcomes:\n",
    "            if parent != child:\n",
    "                learner.addForbiddenArc(parent, child)\n",
    "\n",
    "    return dropout_vars, true_outcomes\n",
    "\n",
    "\n",
    "def build_bn(df, outcomes, layer_map, type_processor, score='K2', use_tabu=True, random_seed=42, enforce_structure=True):\n",
    "\n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Step 0: Exclude 'L4 \u2013 Potential disease process markers'\n",
    "    layer_map_filtered = {k: v for k, v in layer_map.items() if k != 'L4 \u2013 Potential disease process markers'}\n",
    "    excluded_vars = layer_map.get('L4 \u2013 Potential disease process markers', [])\n",
    "\n",
    "    # Step 1: Drop excluded outcome variables and excluded biomarkers\n",
    "    outcomes_all_layers = [v for k, v in layer_map.items() if \"Outcomes\" in k or \"Dropout\" in k]\n",
    "    outcomes_flat = [var for sublist in outcomes_all_layers for var in sublist]\n",
    "    drop_outcomes = list(set(outcomes_flat) - set(outcomes))\n",
    "\n",
    "    df_reduced = df.drop(columns=drop_outcomes + excluded_vars, errors='ignore')\n",
    "    variables_to_keep = df_reduced.columns.tolist()\n",
    "\n",
    "    # Step 2: Build template from reduced dataframe\n",
    "    template_reduced = type_processor.discretizedTemplate(df_reduced)\n",
    "\n",
    "    # Step 3: Instantiate the learner\n",
    "    learner = gum.BNLearner(df_reduced, template_reduced)\n",
    "\n",
    "    if score.upper() == 'K2':\n",
    "        learner.useScoreK2()\n",
    "    elif score.upper() == 'BIC':\n",
    "        learner.useScoreBIC()\n",
    "        learner.useSmoothingPrior()\n",
    "    elif score.upper() == \"BDEU\":\n",
    "        learner.useScoreBDeu(ess=1)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported score method: choose 'K2' or 'BIC'\")\n",
    "\n",
    "    if use_tabu:\n",
    "        learner.useLocalSearchWithTabuList()\n",
    "    else:\n",
    "        learner.useGreedyHillClimbing()\n",
    "\n",
    "    # Step 4: Optionally apply guided structure constraints\n",
    "    dropout_vars, true_outcomes = configure_guided_structure(\n",
    "        learner,\n",
    "        layer_map_filtered,\n",
    "        variables_to_keep,\n",
    "        outcomes,\n",
    "        enforce=enforce_structure,\n",
    "    )\n",
    "\n",
    "    learner.setMaxIndegree(5)\n",
    "    # Step 7: Learn the network\n",
    "    bn = learner.learnBN()\n",
    "    # Step 8: Ensure outcomes point to dropout layer when enforcing guidance\n",
    "    if enforce_structure:\n",
    "        try:\n",
    "            dropout_ids = [bn.idFromName(name) for name in dropout_vars if name in bn.names()]\n",
    "            outcome_ids = [bn.idFromName(name) for name in true_outcomes if name in bn.names()]\n",
    "            for drop_id in dropout_ids:\n",
    "                for out_id in outcome_ids:\n",
    "                    if not bn.existsArc(out_id, drop_id):\n",
    "                        bn.addArc(out_id, drop_id)\n",
    "        except gum.InvalidArgument:\n",
    "            pass\n",
    "\n",
    "    return bn\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_cdr = build_bn(df_imp, outcomes=[\"OUTCOME_CDR_INCREASE\", \"DROPOUT REASON\"], layer_map=layer_map, type_processor=type_processor, score=\"K2\")\n",
    "bn_mace = build_bn(df_imp, outcomes=[\"OUTCOME_MACE\", \"DROPOUT REASON\"], layer_map=layer_map, type_processor=type_processor, score=\"K2\")\n",
    "bn_joint = build_bn(df_imp, outcomes=[\"OUTCOME_CDR_INCREASE\", \"OUTCOME_MACE\", \"DROPOUT REASON\"], layer_map=layer_map, type_processor=type_processor, score=\"K2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define your ordered layers and assign them increasing color intensity\n",
    "layer_order = [\n",
    "    'L0 \u2013 Unmodifiable demographics',\n",
    "    'L1 \u2013 Modifiable demograhpics / Lifestyle factors',\n",
    "    'L2 \u2013 Cardiovascular risk factors',\n",
    "    'L4 \u2013 Potential disease process markers',\n",
    "    'L5 - Imaging markers of neurovascular damage',\n",
    "    'L6 \u2013 Current and previous cardiovascular diagnoses / Vascular interventions',\n",
    "    'L7 - Functional status',\n",
    "    'L8 \u2013 Outcomes',\n",
    "    'L9 \u2013 Dropout'\n",
    "]\n",
    "\n",
    "# Set adjusted min and max color values\n",
    "min_color_val = 0.111111\n",
    "max_color_val = 0.99999\n",
    "steps = len(layer_order) - 1\n",
    "\n",
    "# Create adjusted color intensity map\n",
    "layer_color_map = {\n",
    "    name: min_color_val + i * (max_color_val - min_color_val) / steps\n",
    "    for i, name in enumerate(layer_order)\n",
    "}\n",
    "\n",
    "# Build the nodeColor dictionary\n",
    "node_colors = {}\n",
    "for layer_name, variables in layer_map.items():\n",
    "    color_val = layer_color_map.get(layer_name, 0.5)  # fallback if unknown\n",
    "    for var in variables:\n",
    "        if var in bn_joint.names():\n",
    "            node_colors[var] = color_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap helper: quantify edge stability across resampled datasets\n",
    "from collections import defaultdict\n",
    "def bootstrap_edge_frequencies(df, outcomes, layer_map, type_processor,\n",
    "                                build_bn_func, n_bootstraps=100, score='K2', use_tabu=True, random_seed=42):\n",
    "    edge_counts = defaultdict(int)\n",
    "\n",
    "    for i in range(n_bootstraps):\n",
    "        seed = random_seed + i\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # Bootstrap resample\n",
    "        boot_df = df.sample(frac=1.0, replace=True, random_state=seed)\n",
    "\n",
    "        try:\n",
    "            bn = build_bn_func(boot_df, outcomes, layer_map, type_processor,\n",
    "                               score=score, use_tabu=use_tabu, random_seed=seed)\n",
    "        except Exception as e:\n",
    "            print(f\"Bootstrap {i} failed: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Count arcs\n",
    "        for parent_id, child_id in bn.arcs():\n",
    "            parent = bn.variable(parent_id).name()\n",
    "            child = bn.variable(child_id).name()\n",
    "            edge_counts[(parent, child)] += 1\n",
    "\n",
    "    # Normalize to frequencies\n",
    "    edge_frequencies = {edge: count / n_bootstraps for edge, count in edge_counts.items()}\n",
    "    return edge_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edge_freqs = bootstrap_edge_frequencies(\n",
    "    df=df_imp,\n",
    "    outcomes=[\"OUTCOME_CDR_INCREASE\", \"OUTCOME_MACE\", \"DROPOUT REASON\"],\n",
    "    layer_map=layer_map,\n",
    "    type_processor=type_processor,\n",
    "    build_bn_func=build_bn,\n",
    "    n_bootstraps=200,\n",
    "    score='K2',\n",
    "    use_tabu=True,\n",
    "    random_seed=42\n",
    ")\n",
    "\n",
    "# Print top edges\n",
    "for edge, freq in sorted(edge_freqs.items(), key=lambda x: -x[1]):\n",
    "    print(f\"{edge[0]} \u2192 {edge[1]}: {freq:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the labels for CDR_INCR\n",
    "cdr_labels = list(bn_joint.variable(bn_joint.idFromName(\"OUTCOME_CDR_INCREASE\")).labels())\n",
    "mace_labels = list(bn_joint.variable(bn_joint.idFromName(\"OUTCOME_MACE\")).labels())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fa0f27a",
   "metadata": {},
   "source": [
    "## 6. Posterior Inference and Edge Stability\n",
    "\n",
    "Summarise how the learned networks behave under different evidence configurations and assess structural robustness."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Posterior inference for CDR\n",
    "\n",
    "Visualise how upstream factors shift when conditioning on `OUTCOME_CDR_INCREASE`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Condition the CDR network on each outcome modality to review the upstream clinical drivers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.sideBySide(\n",
    "    *[\n",
    "        gnb.showBN(bn_joint, size=\"9\", nodeColor=node_colors, cmapNode=plt.get_cmap(\"coolwarm\"))\n",
    "    ],\n",
    "    captions=[f\"Inference given that CDR_INCREASE = {label}\" for label in cdr_labels],\n",
    "    ncols=1\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2 Joint inference for CDR and MACE\n",
    "\n",
    "Inspect posterior behaviour when conditioning on both outcome nodes simultaneously."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Focus on the parents of each outcome to summarise the conditional probability tables."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference engine\n",
    "ie = gum.LazyPropagation(bn_joint)\n",
    "ie.makeInference()\n",
    "\n",
    "# Compute MI and apply it to arcs using variable indices (not names)\n",
    "arc_width = {}\n",
    "\n",
    "for u, v in bn_joint.arcs():  # u and v are variable IDs\n",
    "    try:\n",
    "        it = gum.InformationTheory(ie, v, [u])  # use IDs directly\n",
    "        mi = it.mutualInformationXY()\n",
    "    except:\n",
    "        mi = 0.1\n",
    "\n",
    "    arc_width[(u, v)] = mi  # \u2705 use indices, not names\n",
    "\n",
    "max_mi = max(arc_width.values())\n",
    "arc_width = {k: (np.sqrt(v) / np.sqrt(max_mi) * 3.0 + 0.5) for k, v in arc_width.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_name_to_id = {bn_joint.variable(i).name(): i for i in range(bn_joint.size())}\n",
    "\n",
    "bootstrap_freq_id = {}\n",
    "\n",
    "for (parent_name, child_name), freq in edge_freqs.items():\n",
    "    try:\n",
    "        parent_id = var_name_to_id[parent_name]\n",
    "        child_id = var_name_to_id[child_name]\n",
    "        bootstrap_freq_id[(parent_id, child_id)] = freq\n",
    "    except KeyError:\n",
    "        continue  # skip if variable not found in network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_label = {\n",
    "    arc: f\"f={int(bootstrap_freq_id.get(arc, 0.0) * 100)}%\"\n",
    "    for arc in bn_joint.arcs()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_color_values = {\n",
    "    arc: bootstrap_freq_id.get(arc, 0.0)\n",
    "    for arc in bn_joint.arcs()\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with MI-based edge widths\n",
    "gnb.showBN(\n",
    "    bn_joint,\n",
    "    arcWidth=arc_width,\n",
    "    nodeColor=node_colors,\n",
    "    arcColor=arc_color_values,\n",
    "    arcLabel=arc_label,\n",
    "    cmapNode=plt.get_cmap(\"coolwarm\"),\n",
    "    cmapArc=plt.get_cmap(\"Blues\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_imp.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with MI-based edge widths\n",
    "gnb.showInference(\n",
    "    bn_joint,\n",
    "    arcWidth=arc_width,\n",
    "    nodeColor=node_colors,\n",
    "    arcColor=arc_color_values,\n",
    "    cmapNode=plt.get_cmap(\"coolwarm\"),\n",
    "    cmapArc=plt.get_cmap(\"Blues\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here, the whole network is considered"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.sideBySide(\n",
    "  \"<H3>OUTCOME CDR INCREASE</H3>\",\n",
    "  \"<H3>OUTCOME MACE</H3>\",\n",
    "  bn_joint.cpt(\"OUTCOME_CDR_INCREASE\"),\n",
    "  bn_joint.cpt(\"OUTCOME_MACE\"),\n",
    "  ncols=2,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3 Uncorrected mutual information \u2014 CDR_INCREASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize inference engine\n",
    "ie = gum.LazyPropagation(bn_joint)\n",
    "\n",
    "# Get all variables in the network except the outcome\n",
    "all_variables = bn_joint.names()\n",
    "variables_to_test = [var for var in all_variables if var != 'OUTCOME_CDR_INCREASE']\n",
    "\n",
    "# Store results\n",
    "mi_results = {}\n",
    "\n",
    "for var in variables_to_test:\n",
    "    try:\n",
    "        it = gum.InformationTheory(ie, 'OUTCOME_CDR_INCREASE', [var])\n",
    "        mi = it.mutualInformationXY()  # Without correction\n",
    "        mi_results[var] = mi\n",
    "        print(f'Mutual Information ({var} -> OUTCOME_CDR_INCREASE): {mi:.6f}')\n",
    "    except Exception as e:\n",
    "        print(f'Error processing variable {var}: {e}')\n",
    "\n",
    "# Optional: sort results\n",
    "sorted_mi = sorted(mi_results.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nVariable Importance Ranking for OUTCOME_CDR_INCREASE (Raw Mutual Information)\")\n",
    "for var, score in sorted_mi:\n",
    "    print(f'{var}: {score:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.4 Corrected mutual information \u2014 CDR_INCREASE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize inference engine\n",
    "ie = gum.LazyPropagation(bn_joint)\n",
    "\n",
    "# Get the parents of OUTCOME_CDR_INCREASE\n",
    "parents = bn_joint.parents(bn_joint.idFromName('OUTCOME_CDR_INCREASE'))\n",
    "parent_names = [bn_joint.variable(p).name() for p in parents]\n",
    "\n",
    "# Get all variables in the network except OUTCOME_CDR_INCREASE and its parents\n",
    "all_variables = bn_joint.names()\n",
    "variables_to_test = [var for var in all_variables if var != 'OUTCOME_CDR_INCREASE' and var not in parent_names]\n",
    "\n",
    "# Initialize dictionary to store results\n",
    "cmi_results = {}\n",
    "\n",
    "# Loop through each variable and compute conditional mutual information\n",
    "for var in variables_to_test:\n",
    "    try:\n",
    "        it = gum.InformationTheory(ie, 'OUTCOME_CDR_INCREASE', [var], parent_names)\n",
    "        cmi = it.mutualInformationXYgivenZ()\n",
    "        cmi_results[var] = cmi\n",
    "        print(f'Corrected MI ({var} -> OUTCOME_CDR_INCREASE | {parent_names}): {cmi:.6f}')\n",
    "    except Exception as e:\n",
    "        print(f'Error processing variable {var}: {e}')\n",
    "\n",
    "# Optional: sort and display ranking\n",
    "sorted_cmi = sorted(cmi_results.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nVariable Importance Ranking for OUTCOME_CDR_INCREASE (Corrected Mutual Information)\")\n",
    "for var, score in sorted_cmi:\n",
    "    print(f'{var}: {score:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.5 Uncorrected mutual information \u2014 EVENT_MACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize inference engine\n",
    "ie = gum.LazyPropagation(bn_joint)\n",
    "\n",
    "# Get all variables in the network except the outcome\n",
    "all_variables = bn_joint.names()\n",
    "variables_to_test = [var for var in all_variables if var != 'OUTCOME_MACE']\n",
    "\n",
    "# Store results\n",
    "mi_results = {}\n",
    "\n",
    "for var in variables_to_test:\n",
    "    try:\n",
    "        it = gum.InformationTheory(ie, 'OUTCOME_MACE', [var])\n",
    "        mi = it.mutualInformationXY()  # Without correction\n",
    "        mi_results[var] = mi\n",
    "        print(f'Mutual Information ({var} -> OUTCOME_MACE): {mi:.6f}')\n",
    "    except Exception as e:\n",
    "        print(f'Error processing variable {var}: {e}')\n",
    "\n",
    "# Optional: sort results\n",
    "sorted_mi = sorted(mi_results.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nVariable Importance Ranking for OUTCOME_MACE (Raw Mutual Information)\")\n",
    "for var, score in sorted_mi:\n",
    "    print(f'{var}: {score:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.6 Corrected mutual information \u2014 EVENT_MACE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize inference engine\n",
    "ie = gum.LazyPropagation(bn_joint)\n",
    "\n",
    "# Get the parents of EVENT_MACE\n",
    "parents = bn_joint.parents(bn_joint.idFromName('OUTCOME_MACE'))\n",
    "parent_names = [bn_joint.variable(p).name() for p in parents]\n",
    "\n",
    "# Get all variables in the network except OUTCOME_MACE and its parents\n",
    "all_variables = bn_joint.names()\n",
    "variables_to_test = [var for var in all_variables if var != 'OUTCOME_MACE' and var not in parent_names]\n",
    "\n",
    "# Initialize dictionary to store results\n",
    "cmi_results = {}\n",
    "\n",
    "# Loop through each variable and compute conditional mutual information\n",
    "for var in variables_to_test:\n",
    "    try:\n",
    "        it = gum.InformationTheory(ie, 'OUTCOME_MACE', [var], parent_names)\n",
    "        cmi = it.mutualInformationXYgivenZ()\n",
    "        cmi_results[var] = cmi\n",
    "        print(f'Corrected MI ({var} -> OUTCOME_MACE | {parent_names}): {cmi:.6f}')\n",
    "    except Exception as e:\n",
    "        print(f'Error processing variable {var}: {e}')\n",
    "\n",
    "# Optional: sort and display ranking\n",
    "sorted_cmi = sorted(cmi_results.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nVariable Importance Ranking for OUTCOME_MACE (Corrected Mutual Information)\")\n",
    "for var, score in sorted_cmi:\n",
    "    print(f'{var}: {score:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Biomarker Layer Ablation\n",
    "\n",
    "Contrast networks learned with and without the specialised biomarker layer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "from itertools import product\n",
    "\n",
    "def build_bn_biomarkers(df, outcomes, layer_map, type_processor, score='K2', use_tabu=True, random_seed=42):\n",
    "    \n",
    "    random.seed(random_seed)\n",
    "    np.random.seed(random_seed)\n",
    "\n",
    "    # Step 1: Drop excluded outcome variables and excluded biomarkers\n",
    "    outcomes_all_layers = [v for k, v in layer_map.items() if \"Outcomes\" in k or \"Dropout\" in k]\n",
    "    outcomes_flat = [var for sublist in outcomes_all_layers for var in sublist]\n",
    "    drop_outcomes = list(set(outcomes_flat) - set(outcomes))\n",
    "\n",
    "    df_reduced = df.drop(columns=drop_outcomes, errors='ignore')\n",
    "    variables_to_keep = df_reduced.columns.tolist()\n",
    "\n",
    "    # Step 2: Build template from reduced dataframe\n",
    "    template_reduced = type_processor.discretizedTemplate(df_reduced)\n",
    "\n",
    "    # Step 3: Instantiate the learner\n",
    "    learner = gum.BNLearner(df_reduced, template_reduced)\n",
    "\n",
    "    if score.upper() == 'K2':\n",
    "        learner.useScoreK2()\n",
    "    elif score.upper() == 'BIC':\n",
    "        learner.useScoreBIC()\n",
    "        learner.useSmoothingPrior()\n",
    "    elif score.upper() == \"BDEU\":\n",
    "        learner.useScoreBDeu(ess=1)\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported score method: choose 'K2' or 'BIC'\")\n",
    "\n",
    "    if use_tabu:\n",
    "        learner.useLocalSearchWithTabuList()\n",
    "    else:\n",
    "        learner.useGreedyHillClimbing()\n",
    "\n",
    "  # Step 4: Build allowed arcs with special handling for dropout layer\n",
    "    layer_keys = list(layer_map.keys())\n",
    "\n",
    "    dropout_layer_keys = [k for k in layer_keys if \"Dropout\" in k]\n",
    "    dropout_vars = []\n",
    "    for key in dropout_layer_keys:\n",
    "        dropout_vars += layer_map[key]\n",
    "\n",
    "    allowed_arcs = []\n",
    "\n",
    "    for i in range(len(layer_keys)):\n",
    "        from_vars = layer_map[layer_keys[i]]\n",
    "        for j in range(i, len(layer_keys)):\n",
    "            to_vars = layer_map[layer_keys[j]]\n",
    "\n",
    "            for parent, child in product(from_vars, to_vars):\n",
    "                # Special rule: only allow arcs into dropout vars if from outcome\n",
    "                if child in dropout_vars:\n",
    "                    if parent in outcomes:\n",
    "                        allowed_arcs.append((parent, child))\n",
    "                    continue  # skip all others going into dropout\n",
    "\n",
    "                if parent in variables_to_keep and child in variables_to_keep:\n",
    "                    allowed_arcs.append((parent, child))\n",
    "\n",
    "    # Step 5: Enforce allowed arcs, forbid the rest\n",
    "    for parent in variables_to_keep:\n",
    "        for child in variables_to_keep:\n",
    "            if parent != child and (parent, child) not in allowed_arcs:\n",
    "                learner.addForbiddenArc(parent, child)\n",
    "\n",
    "    # Step 6: Disallow arcs between actual outcomes, but allow outcomes \u2192 dropout\n",
    "    true_outcomes = [var for var in outcomes if \"DROPOUT\" not in var.upper()]\n",
    "    for parent in true_outcomes:\n",
    "        for child in true_outcomes:\n",
    "            if parent != child:\n",
    "                learner.addForbiddenArc(parent, child)\n",
    "\n",
    "    learner.setMaxIndegree(5)\n",
    "    # Step 7: Learn the network\n",
    "    bn = learner.learnBN()\n",
    "    return bn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bn_with_biomarkers = build_bn_biomarkers(df_imp, [\"OUTCOME_CDR_INCREASE\", \"OUTCOME_MACE\", \"DROPOUT REASON\"], layer_map=layer_map, type_processor=type_processor, score=\"K2\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7.1 Biomarker-outcome logistic regression\n",
    "\n",
    "Quantify adjusted associations between each biomarker and the clinical outcomes using logistic regression models with age and sex as covariates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.formula.api as smf\n",
    "\n",
    "biomarker_vars = [\n",
    "    var for var in layer_map.get('L4 \u2013 Potential disease process markers', [])\n",
    "    if var in df_imp.columns\n",
    "]\n",
    "outcome_labels = {\n",
    "    'OUTCOME_MACE': 'MACE event',\n",
    "    'OUTCOME_CDR_INCREASE': 'CDR increase'\n",
    "}\n",
    "\n",
    "logit_rows = []\n",
    "\n",
    "for outcome, label in outcome_labels.items():\n",
    "    observed = df_imp[df_imp[outcome].isin(['Yes', 'No'])].copy()\n",
    "    if observed.empty:\n",
    "        continue\n",
    "\n",
    "    outcome_bin = f\"{outcome}_bin\"\n",
    "    observed[outcome_bin] = (observed[outcome] == 'Yes').astype(int)\n",
    "\n",
    "    for biomarker in biomarker_vars:\n",
    "        biomarker_term = f'Q(\"{biomarker}\")'\n",
    "        model_specs = [\n",
    "            {\n",
    "                \"label\": \"Unadjusted\",\n",
    "                \"required_vars\": [outcome_bin, biomarker],\n",
    "                \"formula_suffix\": \"\"\n",
    "            },\n",
    "            {\n",
    "                \"label\": \"Adjusted (age + sex)\",\n",
    "                \"required_vars\": [outcome_bin, biomarker, 'AGE', 'SEX'],\n",
    "                \"formula_suffix\": \" + AGE + C(SEX)\"\n",
    "            },\n",
    "        ]\n",
    "\n",
    "        for spec in model_specs:\n",
    "            missing_covars = [col for col in spec[\"required_vars\"] if col not in observed.columns]\n",
    "            if missing_covars:\n",
    "                logit_rows.append({\n",
    "                    'Outcome': label,\n",
    "                    'Biomarker': biomarker,\n",
    "                    'Model': spec[\"label\"],\n",
    "                    'Odds Ratio': np.nan,\n",
    "                    'CI 2.5%': np.nan,\n",
    "                    'CI 97.5%': np.nan,\n",
    "                    'p-value': np.nan,\n",
    "                    'N': 0,\n",
    "                    'Notes': f\"Missing covariates: {', '.join(missing_covars)}\"\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            model_df = observed[spec[\"required_vars\"]].dropna()\n",
    "            sample_size = int(model_df.shape[0])\n",
    "\n",
    "            if sample_size == 0:\n",
    "                logit_rows.append({\n",
    "                    'Outcome': label,\n",
    "                    'Biomarker': biomarker,\n",
    "                    'Model': spec[\"label\"],\n",
    "                    'Odds Ratio': np.nan,\n",
    "                    'CI 2.5%': np.nan,\n",
    "                    'CI 97.5%': np.nan,\n",
    "                    'p-value': np.nan,\n",
    "                    'N': sample_size,\n",
    "                    'Notes': 'No complete cases after filtering'\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            if model_df[outcome_bin].nunique() < 2 or model_df[biomarker].nunique() < 2:\n",
    "                logit_rows.append({\n",
    "                    'Outcome': label,\n",
    "                    'Biomarker': biomarker,\n",
    "                    'Model': spec[\"label\"],\n",
    "                    'Odds Ratio': np.nan,\n",
    "                    'CI 2.5%': np.nan,\n",
    "                    'CI 97.5%': np.nan,\n",
    "                    'p-value': np.nan,\n",
    "                    'N': sample_size,\n",
    "                    'Notes': 'Insufficient variation'\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            suffix = spec[\"formula_suffix\"]\n",
    "            formula = f\"{outcome_bin} ~ {biomarker_term}{suffix}\"\n",
    "\n",
    "            try:\n",
    "                fit = smf.logit(formula=formula, data=model_df).fit(disp=0)\n",
    "            except Exception as exc:\n",
    "                logit_rows.append({\n",
    "                    'Outcome': label,\n",
    "                    'Biomarker': biomarker,\n",
    "                    'Model': spec[\"label\"],\n",
    "                    'Odds Ratio': np.nan,\n",
    "                    'CI 2.5%': np.nan,\n",
    "                    'CI 97.5%': np.nan,\n",
    "                    'p-value': np.nan,\n",
    "                    'N': sample_size,\n",
    "                    'Notes': f'Fit failed: {exc}'\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            param_name = biomarker_term\n",
    "\n",
    "            if param_name not in fit.params.index:\n",
    "                logit_rows.append({\n",
    "                    'Outcome': label,\n",
    "                    'Biomarker': biomarker,\n",
    "                    'Model': spec[\"label\"],\n",
    "                    'Odds Ratio': np.nan,\n",
    "                    'CI 2.5%': np.nan,\n",
    "                    'CI 97.5%': np.nan,\n",
    "                    'p-value': np.nan,\n",
    "                    'N': sample_size,\n",
    "                    'Notes': 'Biomarker term dropped during fitting'\n",
    "                })\n",
    "                continue\n",
    "\n",
    "            conf = fit.conf_int().loc[param_name]\n",
    "            logit_rows.append({\n",
    "                'Outcome': label,\n",
    "                'Biomarker': biomarker,\n",
    "                'Model': spec[\"label\"],\n",
    "                'Odds Ratio': np.exp(fit.params[param_name]),\n",
    "                'CI 2.5%': np.exp(conf[0]),\n",
    "                'CI 97.5%': np.exp(conf[1]),\n",
    "                'p-value': fit.pvalues[param_name],\n",
    "                'N': sample_size,\n",
    "                'Notes': ''\n",
    "            })\n",
    "\n",
    "logit_results = pd.DataFrame(logit_rows)\n",
    "\n",
    "if not logit_results.empty:\n",
    "    display_order = ['Unadjusted', 'Adjusted (age + sex)']\n",
    "    formatted = logit_results.copy()\n",
    "    formatted['Model'] = pd.Categorical(formatted['Model'], categories=display_order, ordered=True)\n",
    "    formatted['Odds Ratio (95% CI)'] = formatted.apply(\n",
    "        lambda row: (\n",
    "            f\"{row['Odds Ratio']:.2f} ({row['CI 2.5%']:.2f}, {row['CI 97.5%']:.2f})\"\n",
    "            if pd.notnull(row['Odds Ratio']) else 'NA'\n",
    "        ),\n",
    "        axis=1\n",
    "    )\n",
    "    formatted['p-value'] = formatted['p-value'].map(\n",
    "        lambda x: f\"{x:.3g}\" if pd.notnull(x) else 'NA'\n",
    "    )\n",
    "    formatted = formatted.sort_values(['Outcome', 'Biomarker', 'Model']).reset_index(drop=True)\n",
    "\n",
    "    wide = (\n",
    "        formatted\n",
    "        .pivot_table(\n",
    "            index=['Outcome', 'Biomarker', 'N'],\n",
    "            columns='Model',\n",
    "            values=['Odds Ratio (95% CI)', 'p-value'],\n",
    "            aggfunc='first'\n",
    "        )\n",
    "        .sort_index()\n",
    "    )\n",
    "\n",
    "    wide.columns = [f\"{metric} ({model})\" for metric, model in wide.columns]\n",
    "    wide = wide.reset_index()\n",
    "    display(wide[['Outcome', 'Biomarker', 'N'] + [col for col in wide.columns if col not in {'Outcome', 'Biomarker', 'N'}]])\n",
    "else:\n",
    "    display(pd.DataFrame({'Message': ['No logistic regression models were fitted.']}))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Biomarker distribution checks\n",
    "\n",
    "Visualise biomarker levels by CDR outcome status to verify the magnitude of group differences suggested by the logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference engine\n",
    "ie = gum.LazyPropagation(bn_with_biomarkers)\n",
    "ie.makeInference()\n",
    "\n",
    "# Compute MI and apply it to arcs using variable indices (not names)\n",
    "arc_width = {}\n",
    "\n",
    "for u, v in bn_with_biomarkers.arcs():  # u and v are variable IDs\n",
    "    try:\n",
    "        it = gum.InformationTheory(ie, v, [u])  # use IDs directly\n",
    "        mi = it.mutualInformationXY()\n",
    "    except:\n",
    "        mi = 0.1\n",
    "\n",
    "    arc_width[(u, v)] = mi  # \u2705 use indices, not names\n",
    "\n",
    "max_mi = max(arc_width.values())\n",
    "arc_width = {k: (np.sqrt(v) / np.sqrt(max_mi) * 3.0 + 0.5) for k, v in arc_width.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "var_name_to_id = {bn_with_biomarkers.variable(i).name(): i for i in range(bn_with_biomarkers.size())}\n",
    "\n",
    "bootstrap_freq_id = {}\n",
    "\n",
    "for (parent_name, child_name), freq in edge_freqs.items():\n",
    "    try:\n",
    "        parent_id = var_name_to_id[parent_name]\n",
    "        child_id = var_name_to_id[child_name]\n",
    "        bootstrap_freq_id[(parent_id, child_id)] = freq\n",
    "    except KeyError:\n",
    "        continue  # skip if variable not found in network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update layer_order for color mapping and arc logic\n",
    "layer_order = [\n",
    "    'L0 \u2013 Unmodifiable demographics',\n",
    "    'L1 \u2013 Modifiable demograhpics / Lifestyle factors',\n",
    "    'L2 \u2013 Cardiovascular risk factors',\n",
    "    'L3 \u2013 Specialized biomarkers',  # <-- NEW LAYER\n",
    "    'L4 \u2013 Potential disease process markers',\n",
    "    'L5 - Imaging markers of neurovascular damage',\n",
    "    'L6 \u2013 Current and previous cardiovascular diagnoses / Vascular interventions',\n",
    "    'L7 - Functional status',\n",
    "    'L8 \u2013 Outcomes',\n",
    "    'L9 \u2013 Dropout'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set adjusted min and max color values\n",
    "min_color_val = 0.111111\n",
    "max_color_val = 0.99999\n",
    "steps = len(layer_order) - 1\n",
    "\n",
    "# Create adjusted color intensity map\n",
    "layer_color_map = {\n",
    "    name: min_color_val + i * (max_color_val - min_color_val) / steps\n",
    "    for i, name in enumerate(layer_order)\n",
    "}\n",
    "\n",
    "# Build the nodeColor dictionary\n",
    "node_colors = {}\n",
    "for layer_name, variables in layer_map.items():\n",
    "    color_val = layer_color_map.get(layer_name, 0.5)  # fallback if unknown\n",
    "    for var in variables:\n",
    "        if var in bn_with_biomarkers.names():\n",
    "            node_colors[var] = color_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "def bootstrap_edge_frequencies(df, outcomes, layer_map, type_processor,\n",
    "                                build_bn_func, n_bootstraps=100, score='K2', use_tabu=True, random_seed=42):\n",
    "    edge_counts = defaultdict(int)\n",
    "\n",
    "    for i in range(n_bootstraps):\n",
    "        seed = random_seed + i\n",
    "        random.seed(seed)\n",
    "        np.random.seed(seed)\n",
    "\n",
    "        # Bootstrap resample\n",
    "        boot_df = df.sample(frac=1.0, replace=True, random_state=seed)\n",
    "\n",
    "        try:\n",
    "            bn = build_bn_func(boot_df, outcomes, layer_map, type_processor,\n",
    "                               score=score, use_tabu=use_tabu, random_seed=seed)\n",
    "        except Exception as e:\n",
    "            print(f\"Bootstrap {i} failed: {e}\")\n",
    "            continue\n",
    "\n",
    "        # Count arcs\n",
    "        for parent_id, child_id in bn.arcs():\n",
    "            parent = bn.variable(parent_id).name()\n",
    "            child = bn.variable(child_id).name()\n",
    "            edge_counts[(parent, child)] += 1\n",
    "\n",
    "    # Normalize to frequencies\n",
    "    edge_frequencies = {edge: count / n_bootstraps for edge, count in edge_counts.items()}\n",
    "    return edge_frequencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Met biomarkers\n",
    "edge_freqs_with = bootstrap_edge_frequencies(\n",
    "    df=df_imp,\n",
    "    outcomes=[\"OUTCOME_CDR_INCREASE\", \"OUTCOME_MACE\", \"DROPOUT REASON\"],\n",
    "    layer_map=layer_map,\n",
    "    type_processor=type_processor,\n",
    "    build_bn_func=build_bn_biomarkers,\n",
    "    n_bootstraps=10,\n",
    "    score='K2',\n",
    "    use_tabu=True\n",
    "\n",
    ")\n",
    "\n",
    "# Print top edges\n",
    "for edge, freq in sorted(edge_freqs_with.items(), key=lambda x: -x[1]):\n",
    "    print(f\"{edge[0]} \u2192 {edge[1]}: {freq:.2f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create inference engine\n",
    "ie = gum.LazyPropagation(bn_with_biomarkers)\n",
    "ie.makeInference()\n",
    "\n",
    "# Compute MI and apply it to arcs using variable indices (not names)\n",
    "arc_width = {}\n",
    "\n",
    "for u, v in bn_with_biomarkers.arcs():  # u and v are variable IDs\n",
    "    try:\n",
    "        it = gum.InformationTheory(ie, v, [u])  # use IDs directly\n",
    "        mi = it.mutualInformationXY()\n",
    "    except:\n",
    "        mi = 0.1\n",
    "\n",
    "    arc_width[(u, v)] = mi  # \u2705 use indices, not names\n",
    "\n",
    "max_mi = max(arc_width.values())\n",
    "arc_width = {k: (np.sqrt(v) / np.sqrt(max_mi) * 3.0 + 0.5) for k, v in arc_width.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use edge_freqs_with for the network with biomarkers!\n",
    "var_name_to_id = {bn_with_biomarkers.variable(i).name(): i for i in range(bn_with_biomarkers.size())}\n",
    "\n",
    "bootstrap_freq_id = {}\n",
    "\n",
    "for (parent_name, child_name), freq in edge_freqs_with.items():  # <-- use edge_freqs_with here!\n",
    "    try:\n",
    "        parent_id = var_name_to_id[parent_name]\n",
    "        child_id = var_name_to_id[child_name]\n",
    "        bootstrap_freq_id[(parent_id, child_id)] = freq\n",
    "    except KeyError:\n",
    "        continue  # skip if variable not found in network\n",
    "\n",
    "arc_color_values = {\n",
    "    arc: bootstrap_freq_id.get(arc, 0.0)\n",
    "    for arc in bn_with_biomarkers.arcs()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with MI-based edge widths\n",
    "gnb.showInference(\n",
    "    bn_with_biomarkers,\n",
    "    arcWidth=arc_width,\n",
    "    nodeColor=node_colors,\n",
    "    arcColor=arc_color_values,\n",
    "    cmapArc=plt.get_cmap(\"Blues\"),\n",
    "    cmapNode=plt.get_cmap(\"coolwarm\")\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arc_label = {\n",
    "    arc: f\"f={int(bootstrap_freq_id.get(arc, 0.0) * 100)}%\"\n",
    "    for arc in bn_with_biomarkers.arcs()\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot with MI-based edge widths\n",
    "gnb.showBN(\n",
    "    bn_with_biomarkers,\n",
    "    arcWidth=arc_width,\n",
    "    nodeColor=node_colors,\n",
    "    arcColor=arc_color_values,\n",
    "    arcLabel=arc_label,\n",
    "    cmapNode=plt.get_cmap(\"coolwarm\"),\n",
    "    cmapArc=plt.get_cmap(\"Blues\")\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The specialised biomarkers appear upstream but are not directly connected to the outcomes; removing the layer leaves predictive behaviour largely unchanged."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.showTensor(bn_with_biomarkers.cpt(bn_with_biomarkers.idFromName(\"OUTCOME_CDR_INCREASE\")), digits=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.showTensor(bn_with_biomarkers.cpt(bn_with_biomarkers.idFromName(\"OUTCOME_MACE\")), digits=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize inference engine\n",
    "ie = gum.LazyPropagation(bn_with_biomarkers)\n",
    "\n",
    "# Get all variables in the network except the outcome\n",
    "all_variables = bn_with_biomarkers.names()\n",
    "variables_to_test = [var for var in all_variables if var != 'OUTCOME_MACE']\n",
    "\n",
    "# Store results\n",
    "mi_results = {}\n",
    "\n",
    "for var in variables_to_test:\n",
    "    try:\n",
    "        it = gum.InformationTheory(ie, 'OUTCOME_MACE', [var])\n",
    "        mi = it.mutualInformationXY()  # Without correction\n",
    "        mi_results[var] = mi\n",
    "        print(f'Mutual Information ({var} -> OUTCOME_MACE): {mi:.6f}')\n",
    "    except Exception as e:\n",
    "        print(f'Error processing variable {var}: {e}')\n",
    "\n",
    "# Optional: sort results\n",
    "sorted_mi = sorted(mi_results.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nVariable Importance Ranking for OUTCOME_MACE (Raw Mutual Information)\")\n",
    "for var, score in sorted_mi:\n",
    "    print(f'{var}: {score:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize inference engine\n",
    "ie = gum.LazyPropagation(bn_with_biomarkers)\n",
    "\n",
    "# Get all variables in the network except the outcome\n",
    "all_variables = bn_with_biomarkers.names()\n",
    "variables_to_test = [var for var in all_variables if var != 'OUTCOME_CDR_INCREASE']\n",
    "\n",
    "# Store results\n",
    "mi_results = {}\n",
    "\n",
    "for var in variables_to_test:\n",
    "    try:\n",
    "        it = gum.InformationTheory(ie, 'OUTCOME_CDR_INCREASE', [var])\n",
    "        mi = it.mutualInformationXY()  # Without correction\n",
    "        mi_results[var] = mi\n",
    "        print(f'Mutual Information ({var} -> OUTCOME_CDR_INCREASE): {mi:.6f}')\n",
    "    except Exception as e:\n",
    "        print(f'Error processing variable {var}: {e}')\n",
    "\n",
    "# Optional: sort results\n",
    "sorted_mi = sorted(mi_results.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nVariable Importance Ranking for OUTCOME_CDR_INCREASE (Raw Mutual Information)\")\n",
    "for var, score in sorted_mi:\n",
    "    print(f'{var}: {score:.6f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize inference engine\n",
    "ie = gum.LazyPropagation(bn_with_biomarkers)\n",
    "\n",
    "# Get the parents of OUTCOME_CDR_INCREASE\n",
    "parents = bn_with_biomarkers.parents(bn_with_biomarkers.idFromName('OUTCOME_CDR_INCREASE'))\n",
    "parent_names = [bn_with_biomarkers.variable(p).name() for p in parents]\n",
    "\n",
    "# Get all variables in the network except OUTCOME_CDR_INCREASE and its parents\n",
    "all_variables = bn_with_biomarkers.names()\n",
    "variables_to_test = [var for var in all_variables if var != 'OUTCOME_CDR_INCREASE' and var not in parent_names]\n",
    "\n",
    "# Initialize dictionary to store results\n",
    "cmi_results = {}\n",
    "\n",
    "# Loop through each variable and compute conditional mutual information\n",
    "for var in variables_to_test:\n",
    "    try:\n",
    "        it = gum.InformationTheory(ie, 'OUTCOME_CDR_INCREASE', [var], parent_names)\n",
    "        cmi = it.mutualInformationXYgivenZ()\n",
    "        cmi_results[var] = cmi\n",
    "        print(f'Corrected MI ({var} -> OUTCOME_CDR_INCREASE | {parent_names}): {cmi:.6f}')\n",
    "    except Exception as e:\n",
    "        print(f'Error processing variable {var}: {e}')\n",
    "\n",
    "# Optional: sort and display ranking\n",
    "sorted_cmi = sorted(cmi_results.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "print(\"\\nVariable Importance Ranking for OUTCOME_CDR_INCREASE (Corrected Mutual Information)\")\n",
    "for var, score in sorted_cmi:\n",
    "    print(f'{var}: {score:.6f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Scenario Exploration\n",
    "\n",
    "Explore illustrative patient trajectories via direct inference and narrative summaries."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8.1 Bootstrapped outcome risks\n",
    "\n",
    "Quantify uncertainty around the scenario-level outcome probabilities by refitting the joint network on bootstrap resamples and re-running inference for each patient profile."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "\n",
    "def bootstrap_scenario_risks(\n",
    "    df,\n",
    "    scenario_profiles,\n",
    "    target_outcomes,\n",
    "    build_bn_func,\n",
    "    layer_map,\n",
    "    type_processor,\n",
    "    n_bootstraps=200,\n",
    "    score='K2',\n",
    "    use_tabu=True,\n",
    "    random_seed=42,\n",
    "):\n",
    "    samples = defaultdict(list)\n",
    "    outcome_state_labels = {}\n",
    "    failed = 0\n",
    "\n",
    "    for i in range(n_bootstraps):\n",
    "        seed = random_seed + i\n",
    "        boot_df = df.sample(frac=1.0, replace=True, random_state=seed)\n",
    "        try:\n",
    "            bn_iter = build_bn_func(\n",
    "                boot_df,\n",
    "                outcomes=['OUTCOME_CDR_INCREASE', 'OUTCOME_MACE', 'DROPOUT REASON'],\n",
    "                layer_map=layer_map,\n",
    "                type_processor=type_processor,\n",
    "                score=score,\n",
    "                use_tabu=use_tabu,\n",
    "                random_seed=seed,\n",
    "            )\n",
    "        except Exception:\n",
    "            failed += 1\n",
    "            continue\n",
    "\n",
    "        for scenario_name, evidence in scenario_profiles:\n",
    "            ie = gum.LazyPropagation(bn_iter)\n",
    "            try:\n",
    "                ie.setEvidence(evidence)\n",
    "            except gum.InvalidArgument:\n",
    "                continue\n",
    "\n",
    "            for outcome_name, outcome_label in target_outcomes:\n",
    "                if outcome_name not in outcome_state_labels:\n",
    "                    var = bn_iter.variable(bn_iter.idFromName(outcome_name))\n",
    "                    outcome_state_labels[outcome_name] = list(var.labels())\n",
    "                posterior = ie.posterior(outcome_name)\n",
    "                for idx_state, state_label in enumerate(outcome_state_labels[outcome_name]):\n",
    "                    samples[(scenario_name, outcome_name, state_label)].append(float(posterior[idx_state]))\n",
    "\n",
    "    rows = []\n",
    "    for (scenario_name, outcome_name, state_label), values in samples.items():\n",
    "        values = np.asarray(values)\n",
    "        values = values[np.isfinite(values)]\n",
    "        if values.size == 0:\n",
    "            continue\n",
    "        rows.append(\n",
    "            {\n",
    "                'Scenario': scenario_name,\n",
    "                'Outcome': dict(target_outcomes).get(outcome_name, outcome_name),\n",
    "                'Category': state_label,\n",
    "                'Mean probability': values.mean(),\n",
    "                'Std': values.std(ddof=1) if values.size > 1 else 0.0,\n",
    "                'CI 2.5%': np.quantile(values, 0.025),\n",
    "                'CI 97.5%': np.quantile(values, 0.975),\n",
    "                'Bootstraps': int(values.size),\n",
    "            }\n",
    "        )\n",
    "\n",
    "    if failed:\n",
    "        print(f\"Warning: {failed} bootstrap fits failed and were skipped.\")\n",
    "    return pd.DataFrame(rows).sort_values(['Scenario', 'Outcome', 'Category']).reset_index(drop=True)\n",
    "\n",
    "healthy_evs = {\n",
    "    'PATIENT GROUP': 'Reference',\n",
    "    'SMALL VESSEL DISEASE SCORE': '0',\n",
    "    'AGE': '58',\n",
    "    'MINI MENTAL STATE EXAMINATION': '29',\n",
    "    'BASELINE CDR': '0',\n",
    "    'SEX': 'Male',\n",
    "}\n",
    "\n",
    "ill_evs = {\n",
    "    'PATIENT GROUP': 'Vascular cognitive impairment',\n",
    "    'SMALL VESSEL DISEASE SCORE': '2',\n",
    "    'AGE': '76',\n",
    "    'BASELINE CDR': '0',\n",
    "    'SEX': 'Female',\n",
    "}\n",
    "\n",
    "scenario_profiles = [\n",
    "    ('Healthy profile', healthy_evs),\n",
    "    ('Ill profile', ill_evs),\n",
    "]\n",
    "\n",
    "target_outcomes = [\n",
    "    ('OUTCOME_CDR_INCREASE', 'CDR increase'),\n",
    "    ('OUTCOME_MACE', 'MACE event'),\n",
    "]\n",
    "\n",
    "scenario_bootstrap_summary = bootstrap_scenario_risks(\n",
    "    df=df_imp,\n",
    "    scenario_profiles=scenario_profiles,\n",
    "    target_outcomes=target_outcomes,\n",
    "    build_bn_func=build_bn,\n",
    "    layer_map=layer_map,\n",
    "    type_processor=type_processor,\n",
    "    n_bootstraps=200,\n",
    "    score='K2',\n",
    "    use_tabu=True,\n",
    "    random_seed=42,\n",
    ")\n",
    "\n",
    "display(scenario_bootstrap_summary)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = gum.LazyPropagation(bn_joint)\n",
    "\n",
    "# Healthy profile\n",
    "healthy_evs = {\n",
    "    'PATIENT GROUP': 'Reference',\n",
    "    'SMALL VESSEL DISEASE SCORE': '0',\n",
    "    'AGE': '58',\n",
    "    'MINI MENTAL STATE EXAMINATION': '29',\n",
    "    'BASELINE CDR': '0',\n",
    "    'SEX': 'Male'\n",
    "}\n",
    "\n",
    "# Calculate probabilities for healthy patient\n",
    "ie.setEvidence(healthy_evs)\n",
    "healthy_posterior = ie.posterior('OUTCOME_CDR_INCREASE')\n",
    "print(\"Healthy Patient - OUTCOME_CDR_INCREASE probabilities:\")\n",
    "healthy_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probabilities for healthy patient\n",
    "ie.setEvidence(healthy_evs)\n",
    "healthy_posterior = ie.posterior('OUTCOME_MACE')\n",
    "print(\"Healthy Patient - OUTCOME_MACE probabilities:\")\n",
    "healthy_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.showInference(bn_joint, \n",
    "                  evs=healthy_evs, \n",
    "                  size=\"10!\", \n",
    "                  nodeColor=node_colors, \n",
    "                  cmapNode=plt.get_cmap(\"coolwarm\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ie = gum.LazyPropagation(bn_joint)\n",
    "\n",
    "# Healthy profile\n",
    "ill_evs = {\n",
    "    'PATIENT GROUP': 'Vascular cognitive impairment',\n",
    "    'SMALL VESSEL DISEASE SCORE': '2',\n",
    "    'AGE': '76',\n",
    "    'BASELINE CDR': '0',\n",
    "    'SEX': 'Female'\n",
    "}\n",
    "\n",
    "\n",
    "# Calculate probabilities for ill patient\n",
    "ie.setEvidence(ill_evs)\n",
    "ill_posterior = ie.posterior('OUTCOME_CDR_INCREASE')\n",
    "print(\"Ill Patient - OUTCOME_CDR_INCREASE probabilities:\")\n",
    "ill_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate probabilities for ill patient\n",
    "ie.setEvidence(ill_evs)\n",
    "ill_posterior = ie.posterior('OUTCOME_MACE')\n",
    "print(\"Ill Patient - OUTCOME_MACE probabilities:\")\n",
    "ill_posterior"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gnb.showInference(bn_joint, evs=ill_evs, size=\"10!\", nodeColor=node_colors, cmapNode=plt.get_cmap(\"coolwarm\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Persistence and Next Steps\n",
    "\n",
    "Export the trained network and outline follow-up visual summaries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Persist the joint network for reuse outside the notebook\n",
    "gum.saveBN(bn_joint, \"bn_joint.bifxml\")  # You can also use .net or .xdsl\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}