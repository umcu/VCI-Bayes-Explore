{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "732a8402",
   "metadata": {},
   "source": [
    "# HART-BREIN Bayesian Network preprocessing pipeline\n",
    "\n",
    "This notebook recreates the original `preprocess_data.py` pipeline in an interactive way.\n",
    "\n",
    "**Goals**\n",
    "\n",
    "- Load raw HART-BREIN data (SPSS + Excel/CSV).\n",
    "- Derive clinical outcomes (MACE, CDR increase, dropout).\n",
    "- Build the Bayesian network metadata table (layers and variable names).\n",
    "- Create:\n",
    "  - `df.parquet` \u2013 cleaned analysis dataset.\n",
    "  - `df_imp.parquet` \u2013 imputed version of `df.parquet`.\n",
    "  - `bn_vars.parquet` \u2013 variable metadata with layers.\n",
    "  - `baseline_table_by_group.(csv|parquet)` \u2013 baseline characteristics per patient group.\n",
    "\n",
    "Use this notebook to:\n",
    "- Adjust paths and settings.\n",
    "- Inspect intermediate data (e.g. `.head()`, `.info()`, distributions).\n",
    "- Verify that each step behaves as expected.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a208e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 1 \u2013 Imports and logging\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "import logging\n",
    "from dataclasses import dataclass\n",
    "from pathlib import Path\n",
    "from typing import Iterable, Mapping, Sequence\n",
    "\n",
    "import numpy as np\n",
    "import yaml\n",
    "import pandas as pd\n",
    "import re\n",
    "from pandas.api.types import CategoricalDtype\n",
    "import pyreadstat\n",
    "from sklearn.experimental import enable_iterative_imputer  # noqa: F401\n",
    "from sklearn.impute import IterativeImputer\n",
    "\n",
    "LOGGER = logging.getLogger(__name__)\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(levelname)s %(message)s\")\n",
    "\n",
    "pd.set_option(\"display.max_columns\", 120)\n",
    "pd.set_option(\"display.width\", 180)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e32a4e4",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "\n",
    "This section replaces the CLI interface of the original script.\n",
    "\n",
    "- Set `PROJECT_ROOT` to the root of your project.\n",
    "- Optionally define:\n",
    "  - `RISK_REGION`: SCORE2 risk region (`\"Low\"`, `\"Moderate\"`, `\"High\"`, `\"Very high\"`).\n",
    "  - `RAW_DIR`, `OUTPUT_DIR`, `CODEBOOK_PATH`.\n",
    "- Optionally load additional path overrides from `config/data_paths.yml`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0f17fdd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2 \u2013 Configuration dataclass\n",
    "\n",
    "@dataclass\n",
    "class PreprocessConfig:\n",
    "    \"\"\"Configuration for the preprocessing run.\"\"\"\n",
    "\n",
    "    project_root: Path\n",
    "    risk_region: str = \"Low\"\n",
    "    raw_dir: Path | None = None\n",
    "    output_dir: Path | None = None\n",
    "    codebook_path: Path | None = None\n",
    "    seed: int = 1234\n",
    "\n",
    "    def __post_init__(self) -> None:\n",
    "        self.project_root = self.project_root.resolve()\n",
    "        self.raw_dir = (self.project_root / \"data\" / \"raw\") if self.raw_dir is None else self.raw_dir\n",
    "        self.output_dir = (self.project_root / \"data\") if self.output_dir is None else self.output_dir\n",
    "        default_codebook = self.project_root / \"data\" / \"meta\" / \"HBC_CODEBOOK_LABELS.xlsx\"\n",
    "        self.codebook_path = default_codebook if self.codebook_path is None else self.codebook_path\n",
    "\n",
    "    def apply_overrides(self, overrides: Mapping[str, str]) -> None:\n",
    "        \"\"\"Apply path/risk_region overrides from a YAML dictionary.\"\"\"\n",
    "\n",
    "        def _as_path(value: str | None) -> Path | None:\n",
    "            if value in (None, \"\"):\n",
    "                return None\n",
    "            path = Path(value)\n",
    "            if not path.is_absolute():\n",
    "                path = (self.project_root / path).resolve()\n",
    "            return path\n",
    "\n",
    "        if not overrides:\n",
    "            return\n",
    "        if \"risk_region\" in overrides:\n",
    "            self.risk_region = str(overrides[\"risk_region\"])\n",
    "        if \"raw_dir\" in overrides:\n",
    "            self.raw_dir = _as_path(overrides[\"raw_dir\"]) or self.raw_dir\n",
    "        if \"output_dir\" in overrides:\n",
    "            self.output_dir = _as_path(overrides[\"output_dir\"]) or self.output_dir\n",
    "        if \"codebook_path\" in overrides:\n",
    "            self.codebook_path = _as_path(overrides[\"codebook_path\"]) or self.codebook_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f6ce2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3 \u2013 Instantiate config (EDITED)\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# 1. Set your project root to the top-level project folder (one level above src)\n",
    "PROJECT_ROOT = Path(\"..\").resolve()  # <-- key change\n",
    "\n",
    "# 2. Base config\n",
    "config = PreprocessConfig(\n",
    "    project_root=PROJECT_ROOT,\n",
    "    risk_region=\"Low\",      # default; will be overwritten by YAML\n",
    "    raw_dir=None,\n",
    "    output_dir=None,\n",
    "    codebook_path=None,\n",
    "    seed=1234,\n",
    ")\n",
    "\n",
    "# 3. Load overrides from YAML: PROJECT_ROOT/config/config.yml\n",
    "default_config_path = config.project_root / \"config\" / \"config.yml\"\n",
    "print(\"Using config file:\", default_config_path)\n",
    "\n",
    "if default_config_path.exists():\n",
    "    with default_config_path.open(\"r\", encoding=\"utf-8\") as fh:\n",
    "        overrides = yaml.safe_load(fh) or {}\n",
    "    LOGGER.info(\"Applying configuration from %s\", default_config_path)\n",
    "    config.apply_overrides(overrides)\n",
    "else:\n",
    "    LOGGER.warning(\"Config file not found: %s\", default_config_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd101d8",
   "metadata": {},
   "source": [
    "## SCORE2 cardiovascular risk function\n",
    "\n",
    "This section is a direct port of the SCORE2 risk computation from the original script.\n",
    "\n",
    "- `score2(...)` computes a 10-year CVD risk percentage for a single row.\n",
    "- `classify=True` returns risk category labels instead of numeric percentages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d27b605",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 \u2013 SCORE2 implementation\n",
    "\n",
    "def score2(\n",
    "    *,\n",
    "    risk_region: str,\n",
    "    age: float,\n",
    "    gender: str,\n",
    "    smoker: float,\n",
    "    systolic_bp: float,\n",
    "    diabetes: float,\n",
    "    total_chol: float,\n",
    "    total_hdl: float,\n",
    "    classify: bool = False,\n",
    ") -> float | str | None:\n",
    "    \"\"\"Compute SCORE2 cardiovascular risk for a single patient.\"\"\"\n",
    "\n",
    "    gender = gender.lower() if isinstance(gender, str) else gender\n",
    "    if gender not in {\"male\", \"female\"}:\n",
    "        return None\n",
    "\n",
    "    scale1 = scale2 = None\n",
    "    rr = risk_region.lower()\n",
    "    if age < 70:\n",
    "        if rr == \"low\" and gender == \"male\":\n",
    "            scale1, scale2 = -0.5699, 0.7476\n",
    "        elif rr == \"low\" and gender == \"female\":\n",
    "            scale1, scale2 = -0.738, 0.7019\n",
    "        elif rr == \"moderate\" and gender == \"male\":\n",
    "            scale1, scale2 = -0.1565, 0.8009\n",
    "        elif rr == \"moderate\" and gender == \"female\":\n",
    "            scale1, scale2 = -0.3143, 0.7701\n",
    "        elif rr == \"high\" and gender == \"male\":\n",
    "            scale1, scale2 = 0.3207, 0.936\n",
    "        elif rr == \"high\" and gender == \"female\":\n",
    "            scale1, scale2 = 0.571, 0.9369\n",
    "        elif rr == \"very high\" and gender == \"male\":\n",
    "            scale1, scale2 = 0.5836, 0.8294\n",
    "        elif rr == \"very high\" and gender == \"female\":\n",
    "            scale1, scale2 = 0.9412, 0.8329\n",
    "    else:\n",
    "        if rr == \"low\" and gender == \"male\":\n",
    "            scale1, scale2 = -0.34, 1.19\n",
    "        elif rr == \"low\" and gender == \"female\":\n",
    "            scale1, scale2 = -0.52, 1.01\n",
    "        elif rr == \"moderate\" and gender == \"male\":\n",
    "            scale1, scale2 = 0.01, 1.25\n",
    "        elif rr == \"moderate\" and gender == \"female\":\n",
    "            scale1, scale2 = -0.1, 1.1\n",
    "        elif rr == \"high\" and gender == \"male\":\n",
    "            scale1, scale2 = 0.08, 1.15\n",
    "        elif rr == \"high\" and gender == \"female\":\n",
    "            scale1, scale2 = 0.38, 1.09\n",
    "        elif rr == \"very high\" and gender == \"male\":\n",
    "            scale1, scale2 = 0.05, 0.7\n",
    "        elif rr == \"very high\" and gender == \"female\":\n",
    "            scale1, scale2 = 0.38, 0.69\n",
    "    if scale1 is None or scale2 is None:\n",
    "        LOGGER.warning(\"Risk region specification required for SCORE2 computation; returning None\")\n",
    "        return None\n",
    "\n",
    "    smoker = float(smoker) if smoker is not None else 0.0\n",
    "    diabetes = float(diabetes) if diabetes is not None else 0.0\n",
    "\n",
    "    def _calc_under70() -> float:\n",
    "        if gender == \"male\":\n",
    "            term = (\n",
    "                0.3742 * (age - 60) / 5\n",
    "                + 0.6012 * smoker\n",
    "                + 0.2777 * (systolic_bp - 120) / 20\n",
    "                + 0.6457 * diabetes\n",
    "                + 0.1458 * (total_chol - 6) / 1\n",
    "                - 0.2698 * (total_hdl - 1.3) / 0.5\n",
    "                - 0.0755 * (age - 60) / 5 * smoker\n",
    "                - 0.0255 * (age - 60) / 5 * (systolic_bp - 120) / 20\n",
    "                - 0.0281 * (age - 60) / 5 * (total_chol - 6) / 1\n",
    "                + 0.0426 * (age - 60) / 5 * (total_hdl - 1.3) / 0.5\n",
    "                - 0.0983 * (age - 60) / 5 * diabetes\n",
    "            )\n",
    "            tmp = 1 - 0.9605 ** np.exp(term)\n",
    "            return 1 - np.exp(-np.exp(scale1 + scale2 * np.log(-np.log(1 - tmp))))\n",
    "        else:\n",
    "            term = (\n",
    "                0.4648 * (age - 60) / 5\n",
    "                + 0.7744 * smoker\n",
    "                + 0.3131 * (systolic_bp - 120) / 20\n",
    "                + 0.8096 * diabetes\n",
    "                + 0.1002 * (total_chol - 6) / 1\n",
    "                - 0.2606 * (total_hdl - 1.3) / 0.5\n",
    "                - 0.1088 * (age - 60) / 5 * smoker\n",
    "                - 0.0277 * (age - 60) / 5 * (systolic_bp - 120) / 20\n",
    "                - 0.0226 * (age - 60) / 5 * (total_chol - 6) / 1\n",
    "                + 0.0613 * (age - 60) / 5 * (total_hdl - 1.3) / 0.5\n",
    "                - 0.1272 * (age - 60) / 5 * diabetes\n",
    "            )\n",
    "            tmp = 1 - 0.9776 ** np.exp(term)\n",
    "            return 1 - np.exp(-np.exp(scale1 + scale2 * np.log(-np.log(1 - tmp))))\n",
    "\n",
    "    def _calc_over70() -> float:\n",
    "        if gender == \"male\":\n",
    "            term = (\n",
    "                0.0634 * (age - 73)\n",
    "                + 0.4245 * diabetes\n",
    "                + 0.3524 * smoker\n",
    "                + 0.0094 * (systolic_bp - 150)\n",
    "                + 0.085 * (total_chol - 6)\n",
    "                - 0.3564 * (total_hdl - 1.4)\n",
    "                - 0.0174 * (age - 73) * diabetes\n",
    "                - 0.0247 * (age - 73) * smoker\n",
    "                - 0.0005 * (age - 73) * (systolic_bp - 150)\n",
    "                + 0.0073 * (age - 73) * (total_chol - 6)\n",
    "                + 0.0091 * (age - 73) * (total_hdl - 1.4)\n",
    "            )\n",
    "            tmp = 1 - 0.7576 ** np.exp(term - 0.0929)\n",
    "            return 1 - np.exp(-np.exp(scale1 + scale2 * np.log(-np.log(1 - tmp))))\n",
    "        else:\n",
    "            term = (\n",
    "                0.0789 * (age - 73)\n",
    "                + 0.601 * diabetes\n",
    "                + 0.4921 * smoker\n",
    "                + 0.0102 * (systolic_bp - 150)\n",
    "                + 0.0605 * (total_chol - 6)\n",
    "                - 0.304 * (total_hdl - 1.4)\n",
    "                - 0.0107 * (age - 73) * diabetes\n",
    "                - 0.0255 * (age - 73) * smoker\n",
    "                - 0.0004 * (age - 73) * (systolic_bp - 150)\n",
    "                - 0.0009 * (age - 73) * (total_chol - 6)\n",
    "                + 0.0154 * (age - 73) * (total_hdl - 1.4)\n",
    "            )\n",
    "            tmp = 1 - 0.8082 ** np.exp(term - 0.229)\n",
    "            return 1 - np.exp(-np.exp(scale1 + scale2 * np.log(-np.log(1 - tmp))))\n",
    "\n",
    "    risk = _calc_under70() if age < 70 else _calc_over70()\n",
    "    if np.isnan(risk):\n",
    "        return None\n",
    "    risk_pct = round(risk * 100, 1)\n",
    "\n",
    "    if classify:\n",
    "        if age < 50:\n",
    "            if risk_pct < 2.5:\n",
    "                return \"Low risk\"\n",
    "            if risk_pct < 7.5:\n",
    "                return \"Moderate risk\"\n",
    "            return \"High risk\"\n",
    "        if age <= 69:\n",
    "            if risk_pct < 5:\n",
    "                return \"Low risk\"\n",
    "            if risk_pct < 10:\n",
    "                return \"Moderate risk\"\n",
    "            return \"High risk\"\n",
    "        if risk_pct < 7.5:\n",
    "            return \"Low risk\"\n",
    "        if risk_pct < 15:\n",
    "            return \"Moderate risk\"\n",
    "        return \"High risk\"\n",
    "    return risk_pct\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3077fd4b",
   "metadata": {},
   "source": [
    "## General data helpers\n",
    "\n",
    "- `read_sav`: SPSS loader.\n",
    "- `apply_value_labels`: applies value labels from SPSS metadata.\n",
    "- `coalesce`: dplyr-style coalesce across columns.\n",
    "- `to_datetime`: parse date columns.\n",
    "- `map_cdr_values`: remap CDR scoring as in original R script.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3adddb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5 \u2013 Basic helpers\n",
    "\n",
    "def read_sav(path: Path) -> tuple[pd.DataFrame, object]:\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(path)\n",
    "    return pyreadstat.read_sav(path)\n",
    "\n",
    "\n",
    "def apply_value_labels(df: pd.DataFrame, meta) -> pd.DataFrame:\n",
    "    \"\"\"Replace numeric codes with SPSS value labels when available.\"\"\"\n",
    "    if meta is None or not getattr(meta, \"value_labels\", None):\n",
    "        return df\n",
    "\n",
    "    result = df.copy()\n",
    "    value_sets = meta.value_labels\n",
    "    variable_map = getattr(meta, \"variable_value_labels\", {})\n",
    "\n",
    "    for column, label_set in variable_map.items():\n",
    "        if column not in result.columns:\n",
    "            continue\n",
    "        if isinstance(label_set, dict):\n",
    "            mapping = label_set\n",
    "        else:\n",
    "            mapping = value_sets.get(label_set, {})\n",
    "        if mapping:\n",
    "            result[column] = result[column].replace(mapping)\n",
    "    return result\n",
    "\n",
    "\n",
    "def coalesce(df: pd.DataFrame, cols: Sequence[str], target: str) -> None:\n",
    "    \"\"\"Replicate dplyr::coalesce behaviour across multiple columns.\"\"\"\n",
    "    series = None\n",
    "    for col in cols:\n",
    "        if col not in df:\n",
    "            continue\n",
    "        series = df[col] if series is None else series.fillna(df[col])\n",
    "    if series is not None:\n",
    "        df[target] = series\n",
    "\n",
    "\n",
    "def to_datetime(df: pd.DataFrame, columns: Iterable[str]) -> None:\n",
    "    for col in columns:\n",
    "        if col in df:\n",
    "            df[col] = pd.to_datetime(df[col], errors=\"coerce\")\n",
    "\n",
    "\n",
    "def map_cdr_values(series: pd.Series) -> pd.Series:\n",
    "    mapping = {1: 0.5, 2: 1.0, 3: 2.0, 4: 3.0}\n",
    "    return series.replace(mapping)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91a874e1",
   "metadata": {},
   "source": [
    "## Outcome derivation\n",
    "\n",
    "This reproduces the outcome logic from the original `build_outcomes` function:\n",
    "\n",
    "- Coalesces duplicated T2/T4 variables.\n",
    "- Derives:\n",
    "  - Stroke events.\n",
    "  - MACE (baseline + follow-up).\n",
    "  - Dropout reasons (cleaned, normalized).\n",
    "  - `OUTCOME_MACE` and `OUTCOME_CDR_INCREASE` with `\"Yes\" / \"No\" / \"Unobserved\"`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5502334",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 \u2013 Outcome construction\n",
    "\n",
    "def build_outcomes(df_fu: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Reproduce the outcome derivations from 01-A.R.\"\"\"\n",
    "    outcome_cols = [\n",
    "        \"patientID\",\n",
    "        \"T0_datumbaselinebezoek_E1_C1\",\n",
    "        \"T2_datumfu2bezoek_E3_C10\",\n",
    "        \"T4_datum_tel_int_E4_C12\",\n",
    "        \"T0_CDR_E1_C1\",\n",
    "        \"T2_CDR_E3_C10\",\n",
    "        \"T2_CDR_E3_C11\",\n",
    "        \"T4_CDR_E4_C12\",\n",
    "        \"T2_reden_geen_deelname_E3_C11\",\n",
    "        \"T4_reden_geen_deelname_E4_C12\",\n",
    "        \"T0_CVA_E1_C1\",\n",
    "        \"T0_hartinfarct_E1_C1\",\n",
    "        \"T0_dotter_E1_C1\",\n",
    "        \"T0_dotter_stent_E1_C1\",\n",
    "        \"T0_bypass_E1_C1\",\n",
    "        \"T0_etalagebenen_E1_C1\",\n",
    "        \"T0_TIA_E1_C1\",\n",
    "        \"T2_CVA_E3_C10\",\n",
    "        \"T2_CVA_E3_C11\",\n",
    "        \"T2_CVA_datum_E3_C10\",\n",
    "        \"T2_CVA_datum_E3_C11\",\n",
    "        \"T2_CVA_type_E3_C10\",\n",
    "        \"T2_CVA_type_E3_C11\",\n",
    "        \"T2_CVA_type_overig_E3_C10\",\n",
    "        \"T2_CVA_type_overig_E3_C11\",\n",
    "        \"T2_cardio_E3_C10\",\n",
    "        \"T2_cardio_E3_C11\",\n",
    "        \"T2_cardio_type_E3_C10\",\n",
    "        \"T2_cardio_type_E3_C11\",\n",
    "        \"T2_cardio_type_overig_E3_C10\",\n",
    "        \"T2_cardio_type_overig_E3_C11\",\n",
    "        \"T2_cardio_datum_E3_C10\",\n",
    "        \"T2_cardio_datum_E3_C11\",\n",
    "        \"T2_1_datum_overlijden_E3_C11\",\n",
    "        \"T2_1_oorzaak_overlijden_E3_C11\",\n",
    "        \"T4_CVA_E4_C12\",\n",
    "        \"T4_CVA_datum_E4_C12\",\n",
    "        \"T4_CVA_type_overig_E4_C12\",\n",
    "        \"T4_cardio_E4_C12\",\n",
    "        \"T4_cardio_type_E4_C12\",\n",
    "        \"T4_cardio_type_overig_E4_C12\",\n",
    "        \"T4_cardio_datum_E4_C12\",\n",
    "        \"T4_1_datum_overlijden_E4_C12\",\n",
    "        \"T4_1_oorzaak_overlijden_E4_C12\",\n",
    "    ]\n",
    "    essential_cols = {\"patientID\", \"T0_CDR_E1_C1\", \"T2_CDR_E3_C10\", \"T4_CDR_E4_C12\"}\n",
    "    missing_essential = [c for c in essential_cols if c not in df_fu]\n",
    "    if missing_essential:\n",
    "        raise KeyError(f\"Essential outcome columns missing from fu dataset: {missing_essential}\")\n",
    "    missing_optional = [c for c in outcome_cols if c not in df_fu]\n",
    "    if missing_optional:\n",
    "        LOGGER.warning(\"Outcome columns missing from fu dataset and will be filled with NaN: %s\", missing_optional)\n",
    "\n",
    "    outcomes = df_fu.reindex(columns=outcome_cols)\n",
    "    date_cols = [c for c in outcomes.columns if \"datum\" in c.lower()]\n",
    "    to_datetime(outcomes, date_cols)\n",
    "\n",
    "    outcomes.rename(\n",
    "        columns={\n",
    "            \"T0_CDR_E1_C1\": \"T0_CDR\",\n",
    "            \"T2_CDR_E3_C10\": \"T2_CDR\",\n",
    "            \"T4_CDR_E4_C12\": \"T4_CDR\",\n",
    "        },\n",
    "        inplace=True,\n",
    "    )\n",
    "    for col in [\"T0_CDR\", \"T2_CDR\", \"T4_CDR\"]:\n",
    "        if col in outcomes:\n",
    "            outcomes[col] = pd.to_numeric(outcomes[col], errors=\"coerce\")\n",
    "    outcomes[\"T4_CDR\"] = map_cdr_values(outcomes.get(\"T4_CDR\", pd.Series(dtype=float, index=outcomes.index)))\n",
    "    outcomes[\"T2_CDR\"] = map_cdr_values(outcomes.get(\"T2_CDR\", pd.Series(dtype=float, index=outcomes.index)))\n",
    "\n",
    "    # Coalesce duplicated follow-up columns\n",
    "    coalesce(outcomes, [\"T2_CVA_E3_C10\", \"T2_CVA_E3_C11\"], \"T2_CVA\")\n",
    "    coalesce(outcomes, [\"T2_CVA_type_E3_C10\", \"T2_CVA_type_E3_C11\"], \"T2_CVA_type\")\n",
    "    coalesce(outcomes, [\"T2_CVA_datum_E3_C10\", \"T2_CVA_datum_E3_C11\"], \"T2_CVA_datum\")\n",
    "    coalesce(outcomes, [\"T2_cardio_E3_C10\", \"T2_cardio_E3_C11\"], \"T2_cardio\")\n",
    "    coalesce(outcomes, [\"T2_cardio_type_E3_C10\", \"T2_cardio_type_E3_C11\"], \"T2_cardio_type\")\n",
    "    coalesce(outcomes, [\"T2_cardio_type_overig_E3_C10\", \"T2_cardio_type_overig_E3_C11\"], \"T2_cardio_type_overig\")\n",
    "    coalesce(outcomes, [\"T2_cardio_datum_E3_C10\", \"T2_cardio_datum_E3_C11\"], \"T2_cardio_datum\")\n",
    "\n",
    "    # Drop original duplicates\n",
    "    drop_cols = [\n",
    "        \"T2_CDR_E3_C11\",\n",
    "        \"T2_CVA_E3_C10\",\n",
    "        \"T2_CVA_E3_C11\",\n",
    "        \"T2_CVA_type_E3_C10\",\n",
    "        \"T2_CVA_type_E3_C11\",\n",
    "        \"T2_CVA_datum_E3_C10\",\n",
    "        \"T2_CVA_datum_E3_C11\",\n",
    "        \"T2_cardio_E3_C10\",\n",
    "        \"T2_cardio_E3_C11\",\n",
    "        \"T2_cardio_type_E3_C10\",\n",
    "        \"T2_cardio_type_E3_C11\",\n",
    "        \"T2_cardio_type_overig_E3_C10\",\n",
    "        \"T2_cardio_type_overig_E3_C11\",\n",
    "        \"T2_cardio_datum_E3_C10\",\n",
    "        \"T2_cardio_datum_E3_C11\",\n",
    "    ]\n",
    "    outcomes.drop(columns=[c for c in drop_cols if c in outcomes], inplace=True)\n",
    "\n",
    "    # Stroke events\n",
    "    t0_cva = pd.to_numeric(outcomes.get(\"T0_CVA_E1_C1\"), errors=\"coerce\")\n",
    "    outcomes[\"T0_Event_Stroke\"] = np.where(t0_cva.isin([1, 2, 3]), 1, 0)\n",
    "\n",
    "    def contains_any(series: pd.Series, patterns: Sequence[str]) -> pd.Series:\n",
    "        pattern = \"|\".join(patterns)\n",
    "        return series.fillna(\"\").str.contains(pattern, case=False, regex=True)\n",
    "\n",
    "    outcomes[\"T2_Event_Stroke\"] = np.where(\n",
    "        outcomes[\"T2_CVA\"].isin([1, 2])\n",
    "        | contains_any(outcomes.get(\"T2_1_oorzaak_overlijden_E3_C11\"), [\"cva\", \"herseninfarct\", \"hersenbloeding\"]),\n",
    "        1,\n",
    "        0,\n",
    "    )\n",
    "    outcomes[\"T4_Event_Stroke\"] = np.where(\n",
    "        outcomes[\"T4_CVA_E4_C12\"].isin([1, 2])\n",
    "        | contains_any(outcomes.get(\"T4_1_oorzaak_overlijden_E4_C12\"), [\"cva\", \"herseninfarct\", \"hersenbloeding\"]),\n",
    "        1,\n",
    "        0,\n",
    "    )\n",
    "    outcomes[\"Event_Stroke\"] = np.where(\n",
    "        (outcomes[\"T2_Event_Stroke\"] == 1) | (outcomes[\"T4_Event_Stroke\"] == 1),\n",
    "        1,\n",
    "        0,\n",
    "    )\n",
    "\n",
    "    # MACE definition\n",
    "    mace_baseline = (\n",
    "        outcomes[\"T0_CVA_E1_C1\"].isin([1, 2, 3])\n",
    "        | (outcomes.get(\"T0_hartinfarct_E1_C1\") == 1)\n",
    "        | (outcomes.get(\"T0_dotter_E1_C1\") == 1)\n",
    "        | (outcomes.get(\"T0_dotter_stent_E1_C1\") == 1)\n",
    "        | (outcomes.get(\"T0_bypass_E1_C1\") == 1)\n",
    "        | (outcomes.get(\"T0_etalagebenen_E1_C1\") == 1)\n",
    "    )\n",
    "    outcomes[\"T0_Event_MACE\"] = np.where(mace_baseline, 1, 0)\n",
    "\n",
    "    def has_event(series):\n",
    "        if series is None:\n",
    "            return pd.Series(False, index=outcomes.index)\n",
    "        if pd.api.types.is_numeric_dtype(series):\n",
    "            return series.isin([1, 2])\n",
    "        series_str = series.astype(str).str.strip().str.casefold()\n",
    "        return series_str.str.startswith(\"ja\")\n",
    "\n",
    "    mace_t2 = (\n",
    "        has_event(outcomes.get(\"T2_CVA\"))\n",
    "        | has_event(outcomes.get(\"T2_cardio\"))\n",
    "        | contains_any(\n",
    "            outcomes.get(\"T2_1_oorzaak_overlijden_E3_C11\"),\n",
    "            [\"myocardinfarct\", \"hersenbloeding\", \"aneurysma\"],\n",
    "        )\n",
    "    )\n",
    "    outcomes[\"T2_Event_MACE\"] = np.where(mace_t2, 1, 0)\n",
    "\n",
    "    mace_t4 = (\n",
    "        has_event(outcomes.get(\"T4_CVA_E4_C12\"))\n",
    "        | has_event(outcomes.get(\"T4_cardio_E4_C12\"))\n",
    "        | contains_any(\n",
    "            outcomes.get(\"T4_1_oorzaak_overlijden_E4_C12\"),\n",
    "            [\n",
    "                \"cva\",\n",
    "                \"herseninfarct\",\n",
    "                \"vaatlijden\",\n",
    "                \"subarach\",\n",
    "                \"hartstilstand\",\n",
    "                \"cardiac arrest\",\n",
    "                \"decompensatio\",\n",
    "                \"hartfalen\",\n",
    "                \"vasculaire\",\n",
    "                \"myocardinfarct\",\n",
    "            ],\n",
    "        )\n",
    "    )\n",
    "    outcomes[\"T4_Event_MACE\"] = np.where(mace_t4, 1, 0)\n",
    "    outcomes[\"OUTCOME_MACE\"] = np.where(\n",
    "        (outcomes[\"T2_Event_MACE\"] == 1) | (outcomes[\"T4_Event_MACE\"] == 1),\n",
    "        1,\n",
    "        0,\n",
    "    )\n",
    "\n",
    "    # Dropout reasons\n",
    "    dropout_labels = {\n",
    "        0: \"Untraceable\",\n",
    "        1: \"Deceased\",\n",
    "        2: \"Too Ill\",\n",
    "        3: \"Moved to Nursing Home\",\n",
    "        4: \"Refusal\",\n",
    "        5: \"Other\",\n",
    "    }\n",
    "    canonical_dropout = {\n",
    "        \"no dropout\": \"No Dropout\",\n",
    "        \"untraceerbaar\": \"Untraceable\",\n",
    "        \"untraceable\": \"Untraceable\",\n",
    "        \"deceased\": \"Deceased\",\n",
    "        \"too ill\": \"Too Ill\",\n",
    "        \"moved to nursing home\": \"Moved to Nursing Home\",\n",
    "        \"refusal\": \"Refusal\",\n",
    "        \"other\": \"Other\",\n",
    "    }\n",
    "    for phase in (\"T2\", \"T4\"):\n",
    "        col = f\"{phase}_reden_geen_deelname_E3_C11\" if phase == \"T2\" else \"T4_reden_geen_deelname_E4_C12\"\n",
    "        new_col = f\"{phase}_dropout_reason\"\n",
    "        series = outcomes.get(col)\n",
    "        if series is None:\n",
    "            outcomes[new_col] = \"No Dropout\"\n",
    "            outcomes[f\"{new_col}_norm\"] = \"no dropout\"\n",
    "            continue\n",
    "        if pd.api.types.is_numeric_dtype(series):\n",
    "            mapped = series.map(dropout_labels)\n",
    "        else:\n",
    "            mapped = series.replace(dropout_labels)\n",
    "        filled = mapped.fillna(series).fillna(\"No Dropout\")\n",
    "\n",
    "        def _translate_dropout(value: str) -> str:\n",
    "            if not isinstance(value, str):\n",
    "                return value\n",
    "            val = value.strip()\n",
    "            if not val:\n",
    "                return \"No Dropout\"\n",
    "            match = re.match(r\"^([0-9])\\s*=\\s*(.*)$\", val)\n",
    "            if match:\n",
    "                num_code = int(match.group(1))\n",
    "                remainder = match.group(2).strip()\n",
    "                return dropout_labels.get(num_code, canonical_dropout.get(remainder.casefold(), remainder))\n",
    "            lower = val.casefold()\n",
    "            replacements = {\n",
    "                \"geen dropout\": \"No Dropout\",\n",
    "                \"geendropout\": \"No Dropout\",\n",
    "                \"no dropout\": \"No Dropout\",\n",
    "                \"ontraceerbaar\": \"Untraceable\",\n",
    "                \"pati\u00ebnt is overleden\": \"Deceased\",\n",
    "                \"pati\u00ebnt is te ziek\": \"Too Ill\",\n",
    "                \"pati\u00ebnt is opgenomen in een verzorgingshuis\": \"Moved to Nursing Home\",\n",
    "                \"pati\u00ebnt is opgenomen in een verzorgingshuis/verpleeghuis\": \"Moved to Nursing Home\",\n",
    "                \"pati\u00ebnt is opgenomen in een verzorgingshuis / verpleeghuis\": \"Moved to Nursing Home\",\n",
    "                \"pati\u00ebnt weigert\": \"Refusal\",\n",
    "                \"deelnemer, naaste en (huis)arts kunnen niet worden getraceerd\": \"Untraceable\",\n",
    "                \"deelnemer kan niet worden getraceerd\": \"Untraceable\",\n",
    "                \"deelnemer is overleden\": \"Deceased\",\n",
    "                \"deelnemer is te ziek\": \"Too Ill\",\n",
    "                \"deelnemer is opgenomen in een verzorgingshuis/verpleeghuis\": \"Moved to Nursing Home\",\n",
    "                \"deelnemer is opgenomen in een verzorgingshuis / verpleeghuis\": \"Moved to Nursing Home\",\n",
    "                \"deelnemer weigert\": \"Refusal\",\n",
    "                \"anders, namelijk\": \"Other\",\n",
    "                \"anders\": \"Other\",\n",
    "            }\n",
    "            for key, replacement in replacements.items():\n",
    "                if lower.startswith(key):\n",
    "                    return replacement\n",
    "            return canonical_dropout.get(lower, val)\n",
    "\n",
    "        translated = filled.apply(_translate_dropout)\n",
    "        outcomes[new_col] = translated\n",
    "        outcomes[f\"{new_col}_norm\"] = translated.astype(str).str.strip().str.casefold()\n",
    "\n",
    "    outcomes[\"OUTCOME_CDR_INCREASE\"] = np.where(\n",
    "        (outcomes[\"T2_CDR\"] > outcomes[\"T0_CDR\"])\n",
    "        | (outcomes[\"T4_CDR\"] > outcomes[\"T0_CDR\"])\n",
    "        | (outcomes[\"T4_CDR\"] > outcomes[\"T2_CDR\"])\n",
    "        | (outcomes[\"T2_dropout_reason\"] == \"Moved to Nursing Home\")\n",
    "        | (outcomes[\"T4_dropout_reason\"] == \"Moved to Nursing Home\"),\n",
    "        1,\n",
    "        0,\n",
    "    )\n",
    "\n",
    "    outcomes.loc[\n",
    "        outcomes[\"OUTCOME_CDR_INCREASE\"].isna() & (outcomes[\"T4_CDR\"] > outcomes[\"T0_CDR\"]),\n",
    "        \"OUTCOME_CDR_INCREASE\",\n",
    "    ] = 1\n",
    "\n",
    "    # Convert outcomes into categorical labels with explicit Unobserved state\n",
    "    norm_t4 = outcomes.get(\n",
    "        \"T4_dropout_reason_norm\",\n",
    "        outcomes[\"T4_dropout_reason\"].astype(str).str.strip().str.casefold(),\n",
    "    )\n",
    "\n",
    "    mask_mace_unobserved = (outcomes[\"OUTCOME_MACE\"] == 0) & (norm_t4 != \"no dropout\")\n",
    "    outcomes.loc[mask_mace_unobserved, \"OUTCOME_MACE\"] = np.nan\n",
    "    outcomes[\"OUTCOME_MACE\"] = (\n",
    "        outcomes[\"OUTCOME_MACE\"].map({1: \"Yes\", 0: \"No\"}).fillna(\"Unobserved\").astype(\"category\")\n",
    "    )\n",
    "\n",
    "    mask_cdr_unobserved = outcomes[\"OUTCOME_CDR_INCREASE\"].isna() | (\n",
    "        (outcomes[\"OUTCOME_CDR_INCREASE\"] == 0) & (norm_t4 != \"no dropout\")\n",
    "    )\n",
    "    outcomes.loc[mask_cdr_unobserved, \"OUTCOME_CDR_INCREASE\"] = np.nan\n",
    "    outcomes[\"OUTCOME_CDR_INCREASE\"] = (\n",
    "        outcomes[\"OUTCOME_CDR_INCREASE\"].map({1: \"Yes\", 0: \"No\"}).fillna(\"Unobserved\").astype(\"category\")\n",
    "    )\n",
    "\n",
    "    norm_cols = [col for col in outcomes.columns if col.endswith(\"_dropout_reason_norm\")]\n",
    "    outcomes.drop(columns=norm_cols, inplace=True)\n",
    "\n",
    "    outcomes_sub = outcomes[[\"patientID\", \"T4_dropout_reason\", \"OUTCOME_MACE\", \"OUTCOME_CDR_INCREASE\"]].copy()\n",
    "\n",
    "    return outcomes, outcomes_sub\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1cdfbc10",
   "metadata": {},
   "source": [
    "## Subset selection and engineered variables\n",
    "\n",
    "- Filters columns according to `bn_vars` (layered codebook).\n",
    "- Adds engineered variables:\n",
    "  - `T0_SYS_BP`, `T0_DIAS_BP`\n",
    "  - `HV_ICV`, `TBV_ICV`, `CBF`\n",
    "  - smoking quantities and `MACE` history.\n",
    "- Then harmonises column names to the BN naming conventions.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ceaca3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7 \u2013 Subset, harmonisation, and metadata setup\n",
    "\n",
    "def prepare_subset(df_fu: pd.DataFrame, bn_vars: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "    bn_vars_filter = bn_vars.loc[bn_vars[\"LAYER\"].notna(), [\"LAYER\", \"VARIABLE NAME\"]].copy()\n",
    "    bn_vars_filter.loc[bn_vars_filter[\"VARIABLE NAME\"] == \"DROPOUT REASON\", \"LAYER\"] = \"L9 \u2013 Dropout\"\n",
    "    extra_vars = [\n",
    "        \"patientID\",\n",
    "        \"T0_patientengroep_E1_C1\",\n",
    "        \"Sex\",\n",
    "        \"T0_Age\",\n",
    "        \"T0_pTau181\",\n",
    "        \"T0_NfL\",\n",
    "        \"T0_GFAP\",\n",
    "        \"T0_A\u03b240\",\n",
    "        \"T0_A\u03b242\",\n",
    "    ]\n",
    "\n",
    "    all_cols = pd.Index(df_fu.columns)\n",
    "    base_names = all_cols.str.replace(r\"_E[0-9]+_C[0-9]+$\", \"\", regex=True)\n",
    "    allowed = bn_vars_filter[\"VARIABLE NAME\"].unique()\n",
    "    matched_cols = [col for col, base in zip(all_cols, base_names) if base in allowed]\n",
    "    final_cols = list(dict.fromkeys(extra_vars + matched_cols))\n",
    "    subset = df_fu.loc[:, [c for c in final_cols if c in df_fu]].copy()\n",
    "\n",
    "    def safe_mean(cols: Sequence[str]) -> pd.Series:\n",
    "        present = [subset[c] for c in cols if c in subset]\n",
    "        if not present:\n",
    "            return pd.Series(np.nan, index=subset.index)\n",
    "        return pd.concat(present, axis=1).mean(axis=1, skipna=True)\n",
    "\n",
    "    subset[\"T0_SYS_BP\"] = safe_mean([\"T0_systolisch_a_E1_C1\", \"T0_systolisch_b_E1_C1\"])\n",
    "    subset[\"T0_DIAS_BP\"] = safe_mean([\"T0_Diastolisch_a_E1_C1\", \"T0_Diastolisch_b_E1_C1\"])\n",
    "    icv = subset.get(\"T0_q_ic_tissue_total_intracranial_volume_ml\")\n",
    "    if icv is not None:\n",
    "        subset[\"T0_HV_ICV\"] = (\n",
    "            safe_mean(\n",
    "                [\n",
    "                    \"T0_i_ic_gm_Hippocampus_left_volume_ml\",\n",
    "                    \"T0_i_ic_gm_Hippocampus_right_volume_ml\",\n",
    "                ]\n",
    "            )\n",
    "            * 100\n",
    "            / icv\n",
    "        )\n",
    "        brain = subset.get(\"T0_q_ic_tissue_total_brain_volume_ml\")\n",
    "        if brain is not None:\n",
    "            subset[\"T0_TBV_ICV\"] = brain * 100 / icv\n",
    "    subset[\"T0_CBF\"] = subset.get(\"T0_i_ic_cbf_GrayMatter_mean_mL100gmin\")\n",
    "\n",
    "    def fill_smoking(primary: str, backup: str) -> pd.Series:\n",
    "        res = subset.get(primary, pd.Series(np.nan, index=subset.index)).copy()\n",
    "        backup_series = subset.get(backup)\n",
    "        if backup_series is not None:\n",
    "            res = res.fillna(backup_series)\n",
    "        return res.fillna(0)\n",
    "\n",
    "    subset[\"T0_roken_hoeveel_jaar_a_E1_C1\"] = fill_smoking(\n",
    "        \"T0_roken_hoeveel_jaar_a_E1_C1\", \"T0_roken_hoeveel_jaar_b_E1_C1\"\n",
    "    )\n",
    "    subset[\"T0_roken_hoeveel_per_dag_a_E1_C1\"] = fill_smoking(\n",
    "        \"T0_roken_hoeveel_per_dag_a_E1_C1\", \"T0_roken_hoeveel_per_dag_b_E1_C1\"\n",
    "    )\n",
    "\n",
    "    def is_one(col: str) -> pd.Series:\n",
    "        series = subset.get(col)\n",
    "        if series is None:\n",
    "            return pd.Series(False, index=subset.index)\n",
    "        series = series.fillna(0)\n",
    "        if pd.api.types.is_numeric_dtype(series):\n",
    "            return series == 1\n",
    "        series_str = series.astype(str).str.strip().str.casefold()\n",
    "        return series_str.isin({\"1\", \"ja\", \"yes\", \"true\"})\n",
    "\n",
    "    subset[\"T0_CAD\"] = np.where(\n",
    "        is_one(\"T0_bypass_E1_C1\") | is_one(\"T0_dotter_E1_C1\") | is_one(\"T0_hartinfarct_E1_C1\"),\n",
    "        1,\n",
    "        0,\n",
    "    )\n",
    "    subset[\"T0_PAD\"] = np.where(is_one(\"T0_etalagebenen_E1_C1\"), 1, 0)\n",
    "    subset[\"MACE\"] = np.where((subset[\"T0_CAD\"] == 1) | (subset[\"T0_PAD\"] == 1), 1, 0)\n",
    "\n",
    "    subset.drop(\n",
    "        columns=[\n",
    "            col\n",
    "            for col in [\n",
    "                \"T0_roken_hoeveel_jaar_b_E1_C1\",\n",
    "                \"T0_roken_hoeveel_per_dag_b_E1_C1\",\n",
    "                \"T0_etalagebenen_E1_C1\",\n",
    "                \"T0_hartinfarct_E1_C1\",\n",
    "                \"T0_bypass_E1_C1\",\n",
    "                \"T0_dotter_E1_C1\",\n",
    "                \"T0_PAD\",\n",
    "                \"T0_CAD\",\n",
    "            ]\n",
    "            if col in subset\n",
    "        ],\n",
    "        inplace=True,\n",
    "    )\n",
    "\n",
    "    return subset, bn_vars_filter\n",
    "\n",
    "\n",
    "def harmonise_columns(df_final: pd.DataFrame, bn_vars_filter: pd.DataFrame) -> None:\n",
    "    bn_vars_filter[\"LAYER\"] = bn_vars_filter[\"LAYER\"].astype(str).str.strip()\n",
    "    bn_vars_filter[\"VARIABLE NAME\"] = bn_vars_filter[\"VARIABLE NAME\"].str.replace(\"T0_\", \"\", regex=False)\n",
    "    df_final.columns = (\n",
    "        df_final.columns.str.replace(\"_E1_C1\", \"\", regex=False)\n",
    "        .str.replace(\"_E1_C6\", \"\", regex=False)\n",
    "        .str.replace(\"T0_\", \"\", regex=False)\n",
    "        .str.replace(\"\u03b2\", \"B\", regex=False)\n",
    "        .str.upper()\n",
    "    )\n",
    "    bn_vars_filter[\"VARIABLE NAME\"] = (\n",
    "        bn_vars_filter[\"VARIABLE NAME\"].str.replace(\"\u03b2\", \"B\", regex=False).str.upper()\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "401c7817",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 8 \u2013 Extra metadata rows, name mapping, label translations\n",
    "\n",
    "EXTRA_LAYER_ROWS = [\n",
    "    (\"L2 \u2013 Cardiovascular risk factors\", \"SYS_BP\"),\n",
    "    (\"L2 \u2013 Cardiovascular risk factors\", \"DIAS_BP\"),\n",
    "    (\"L5 - Imaging markers of neurovascular damage\", \"HV_ICV\"),\n",
    "    (\"L5 - Imaging markers of neurovascular damage\", \"TBV_ICV\"),\n",
    "    (\"L4 \u2013 Potential disease process markers\", \"CBF\"),\n",
    "    (\"L8 \u2013 Outcomes\", \"OUTCOME_CDR_INCREASE\"),\n",
    "    (\"L9 \u2013 Dropout\", \"DROPOUT REASON\"),\n",
    "    (\"L8 \u2013 Outcomes\", \"OUTCOME_MACE\"),\n",
    "    (\"L0 \u2013 Unmodifiable demographics\", \"AGE\"),\n",
    "    (\"L0 \u2013 Unmodifiable demographics\", \"SEX\"),\n",
    "    (\"L4 \u2013 Potential disease process markers\", \"PTAU181\"),\n",
    "    (\"L4 \u2013 Potential disease process markers\", \"NFL\"),\n",
    "    (\"L4 \u2013 Potential disease process markers\", \"GFAP\"),\n",
    "    (\"L6 \u2013 Current and previous cardiovascular diagnoses / Vascular interventions\", \"PATIENTENGROEP\"),\n",
    "    (\"L4 \u2013 Potential disease process markers\", \"AB40\"),\n",
    "    (\"L4 \u2013 Potential disease process markers\", \"AB42\"),\n",
    "    (\"L6 \u2013 Current and previous cardiovascular diagnoses / Vascular interventions\", \"MACE\"),\n",
    "    (\"L2 \u2013 Cardiovascular risk factors\", \"SCORE_2\"),\n",
    "]\n",
    "\n",
    "\n",
    "NAME_MAPPING = {\n",
    "    \"MMSE_TOTAAL\": \"MINI MENTAL STATE EXAMINATION\",\n",
    "    \"STARKSTEIN\": \"STARKSTEIN SCORE\",\n",
    "    \"CDR\": \"BASELINE CDR\",\n",
    "    \"CVA\": \"STROKE HISTORY\",\n",
    "    \"BCS_1\": \"BIOMARKER SCORE 1\",\n",
    "    \"BCS_2\": \"BIOMARKER SCORE 2\",\n",
    "    \"NEURORAD_SVD_SCORE\": \"SMALL VESSEL DISEASE SCORE\",\n",
    "    \"HV_ICV\": \"HIPPOCAMPUS/INTRACRANIAL VOLUME\",\n",
    "    \"TBV_ICV\": \"BRAIN/INTRACRANIAL VOLUME\",\n",
    "    \"CBF\": \"CEREBRAL BLOOD FLOW\",\n",
    "    \"EVENT_MACE\": \"OUTCOME_MACE\",\n",
    "    \"CDR_INCR\": \"OUTCOME_CDR_INCREASE\",\n",
    "    \"T4_DROPOUT_REASON\": \"DROPOUT REASON\",\n",
    "    \"AGE\": \"AGE\",\n",
    "    \"SEX\": \"SEX\",\n",
    "    \"PTAU181\": \"PTAU181\",\n",
    "    \"NFL\": \"NFL\",\n",
    "    \"GFAP\": \"GFAP\",\n",
    "    \"PATIENTENGROEP\": \"PATIENT GROUP\",\n",
    "    \"MACE\": \"ATHEROSCLEROTIC CARDIOVASCULAR DISEASE HISTORY\",\n",
    "    \"SCORE_2\": \"VASCULAR RISK SCORE\",\n",
    "}\n",
    "\n",
    "\n",
    "LABEL_TRANSLATION: Mapping[str, Mapping[str, str]] = {\n",
    "    \"STROKE HISTORY\": {\n",
    "        \"Ja, hersenbloeding\": \"Yes, hemorrhagic stroke\",\n",
    "        \"Ja, herseninfarct\": \"Yes, ischemic stroke\",\n",
    "        \"Ja, type onbekend\": \"Yes, type unknown\",\n",
    "        \"Nee\": \"No\",\n",
    "    },\n",
    "    \"PATIENT GROUP\": {\n",
    "        \"Carotid occlusive disease\": \"Carotid occlusive disease\",\n",
    "        \"Controle\": \"Reference\",\n",
    "        \"Hartfalen\": \"Heart failure\",\n",
    "        \"Vascular cognitive impairment\": \"Vascular cognitive impairment\",\n",
    "    },\n",
    "    \"ATHEROSCLEROTIC CARDIOVASCULAR DISEASE HISTORY\": {\"0\": \"No\", \"1\": \"Yes\"},\n",
    "}\n",
    "\n",
    "BASELINE_CDR_CATEGORIES = [\"0\", \"0.5\", \"1\", \"1.5\", \"2\", \"3\"]\n",
    "\n",
    "\n",
    "def enforce_domain_categories(df: pd.DataFrame) -> None:\n",
    "    if \"BASELINE CDR\" in df:\n",
    "        def _format_cdr(value):\n",
    "            if pd.isna(value):\n",
    "                return pd.NA\n",
    "            try:\n",
    "                numeric = float(value)\n",
    "            except (TypeError, ValueError):\n",
    "                return str(value)\n",
    "            mapping = {0.0: \"0\", 0.5: \"0.5\", 1.0: \"1\", 1.5: \"1.5\", 2.0: \"2\", 3.0: \"3\"}\n",
    "            return mapping.get(numeric, str(numeric))\n",
    "\n",
    "        formatted = df[\"BASELINE CDR\"].apply(_format_cdr)\n",
    "        df[\"BASELINE CDR\"] = pd.Categorical(formatted, categories=BASELINE_CDR_CATEGORIES, ordered=True)\n",
    "\n",
    "    for column in [\"STROKE HISTORY\", \"PATIENT GROUP\", \"SEX\"]:\n",
    "        if column in df:\n",
    "            series = df[column].astype(\"string\")\n",
    "            df[column] = pd.Categorical(series, ordered=False)\n",
    "\n",
    "    if \"ATHEROSCLEROTIC CARDIOVASCULAR DISEASE HISTORY\" in df:\n",
    "        series = df[\"ATHEROSCLEROTIC CARDIOVASCULAR DISEASE HISTORY\"].astype(\"string\").str.strip()\n",
    "        mapping = {\n",
    "            \"0\": \"No\",\n",
    "            \"1\": \"Yes\",\n",
    "            \"No\": \"No\",\n",
    "            \"Yes\": \"Yes\",\n",
    "        }\n",
    "        translated = series.replace(mapping)\n",
    "        df[\"ATHEROSCLEROTIC CARDIOVASCULAR DISEASE HISTORY\"] = pd.Categorical(\n",
    "            translated,\n",
    "            categories=[\"No\", \"Yes\"],\n",
    "            ordered=False,\n",
    "        )\n",
    "\n",
    "\n",
    "def normalise_string_categories(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for col in df.select_dtypes(include=\"object\"):\n",
    "        df[col] = df[col].astype(str).str.strip()\n",
    "    return df\n",
    "\n",
    "\n",
    "def translate_labels(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    for column, mapping in LABEL_TRANSLATION.items():\n",
    "        if column in df:\n",
    "            series = df[column].astype(str)\n",
    "            lower = series.str.strip().str.casefold()\n",
    "            normalized_mapping = {key.casefold(): value for key, value in mapping.items()}\n",
    "            mapped = lower.map(normalized_mapping)\n",
    "            df[column] = series.where(mapped.isna(), mapped)\n",
    "            df[column] = df[column].astype(\"category\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b568bc8",
   "metadata": {},
   "source": [
    "## Imputation and baseline tables\n",
    "\n",
    "- `impute_dataframe`: IterativeImputer for numeric, mode imputation for categorical.\n",
    "- Baseline table helpers:\n",
    "  - Group by patient group.\n",
    "  - Summarise medians/IQR and percentages.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "972b4c2c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 9 \u2013 Imputation and baseline table helpers\n",
    "\n",
    "def impute_dataframe(df: pd.DataFrame, seed: int) -> pd.DataFrame:\n",
    "    numeric = df.select_dtypes(include=[\"number\"]).copy()\n",
    "    other = df.select_dtypes(exclude=[\"number\"]).copy()\n",
    "\n",
    "    imputer = IterativeImputer(random_state=seed, sample_posterior=False)\n",
    "    if not numeric.empty:\n",
    "        numeric = pd.DataFrame(imputer.fit_transform(numeric), columns=numeric.columns, index=df.index)\n",
    "\n",
    "    for col in other.columns:\n",
    "        series = other[col]\n",
    "        if series.isnull().any():\n",
    "            if pd.api.types.is_categorical_dtype(series):\n",
    "                modes = series.mode(dropna=True)\n",
    "                fill_value = modes.iloc[0] if not modes.empty else series.cat.categories[0]\n",
    "                if fill_value not in series.cat.categories:\n",
    "                    series = series.cat.add_categories([fill_value])\n",
    "                series = series.fillna(fill_value)\n",
    "            else:\n",
    "                modes = series.mode(dropna=True)\n",
    "                fill_value = modes.iloc[0] if not modes.empty else \"\"\n",
    "                series = series.fillna(fill_value)\n",
    "            other[col] = series\n",
    "\n",
    "    combined = pd.concat([numeric, other], axis=1)\n",
    "    return combined[df.columns]\n",
    "\n",
    "\n",
    "BASELINE_GROUP_ORDER = [\n",
    "    \"Carotid occlusive disease\",\n",
    "    \"Heart failure\",\n",
    "    \"Vascular cognitive impairment\",\n",
    "    \"Reference\",\n",
    "]\n",
    "\n",
    "DIABETES_CATEGORY_LABELS = [\n",
    "    (\"Yes, with lifestyle advice\", [\"yes, with lifestyle advice\", \"ja, met leefstijladvies\", \"ja, met leefstijl advies\"]),\n",
    "    (\"Yes, with oral antidiabetics\", [\"yes, with oral antidiabetics\", \"ja, met orale antidiabetica\", \"ja, met orale antidiabetics\"]),\n",
    "    (\"Yes, with insulin\", [\"yes, with insulin\", \"ja, met insuline\"]),\n",
    "    (\"No\", [\"no\", \"nee\", \"geen diabetes\"]),\n",
    "]\n",
    "\n",
    "SMOKING_CATEGORY_LABELS = [\n",
    "    (\"Yes\", [\"yes\", \"ja\", \"smoker\", \"current smoker\"]),\n",
    "    (\"No\", [\"no\", \"nee\", \"never\", \"nooit gerookt\", \"never smoked\"]),\n",
    "    (\"Not anymore\", [\"not anymore\", \"niet meer\", \"gestopt\", \"gestopt met roken\", \"ex-roker\", \"ex roker\", \"ex-smoker\", \"former smoker\", \"quit\"]),\n",
    "]\n",
    "\n",
    "CVA_CATEGORY_LABELS = [\n",
    "    (\"Yes, hemorrhage\", [\"yes, hemorrhage\", \"yes, haemorrhage\", \"yes, hemorrhagic stroke\"]),\n",
    "    (\"Yes, stroke\", [\"yes, stroke\", \"yes, ischemic stroke\"]),\n",
    "    (\"Yes, type unknown\", [\"yes, type unknown\"]),\n",
    "    (\"No\", [\"no\"]),\n",
    "]\n",
    "\n",
    "PATIENT_GROUP_CATEGORY_LABELS = [\n",
    "    (\"Carotid occlusive disease\", [\"carotid occlusive disease\"]),\n",
    "    (\"Heart failure\", [\"heart failure\"]),\n",
    "    (\"Vascular cognitive impairment\", [\"vascular cognitive impairment\"]),\n",
    "    (\"Reference\", [\"reference\", \"controle\"]),\n",
    "]\n",
    "\n",
    "\n",
    "def _normalize_label(value: str | float | int | None) -> str | None:\n",
    "    if value is None or (isinstance(value, str) and value.strip() == \"\"):\n",
    "        return None\n",
    "    if isinstance(value, (float, int)) and pd.isna(value):\n",
    "        return None\n",
    "    return str(value).strip().casefold()\n",
    "\n",
    "\n",
    "def _format_median_iqr(series: pd.Series | None) -> str:\n",
    "    if series is None:\n",
    "        return \"NA\"\n",
    "    numeric = pd.to_numeric(series, errors=\"coerce\").dropna()\n",
    "    if numeric.empty:\n",
    "        return \"NA\"\n",
    "    q1 = numeric.quantile(0.25)\n",
    "    median = numeric.median()\n",
    "    q3 = numeric.quantile(0.75)\n",
    "    return f\"{median:.2f} [{q1:.2f}, {q3:.2f}]\"\n",
    "\n",
    "\n",
    "def _format_count_pct(series: pd.Series | None, targets: Sequence[str]) -> str:\n",
    "    if series is None:\n",
    "        return \"NA\"\n",
    "    valid = pd.Series(series).dropna()\n",
    "    if valid.empty:\n",
    "        return \"NA\"\n",
    "    normalized = valid.apply(_normalize_label).dropna()\n",
    "    if normalized.empty:\n",
    "        return \"NA\"\n",
    "    target_norms = {_normalize_label(value) for value in targets if value is not None}\n",
    "    if not target_norms:\n",
    "        return \"NA\"\n",
    "    count = normalized.isin(target_norms).sum()\n",
    "    pct = (count / len(normalized)) * 100\n",
    "    return f\"{int(count)} ({pct:.1f})\"\n",
    "\n",
    "\n",
    "def _build_group_sequence(df: pd.DataFrame, group_col: str) -> list[tuple[str, pd.DataFrame]]:\n",
    "    groups: list[tuple[str, pd.DataFrame]] = [(\"Overall\", df)]\n",
    "    if group_col not in df:\n",
    "        LOGGER.warning(\"Baseline table: column %s is missing\", group_col)\n",
    "        return groups\n",
    "    present_values = df[group_col].dropna()\n",
    "    added: set[str] = set()\n",
    "    for label in BASELINE_GROUP_ORDER:\n",
    "        mask = present_values == label\n",
    "        if mask.any():\n",
    "            groups.append((label, df[df[group_col] == label]))\n",
    "            added.add(label)\n",
    "    for value in present_values.unique():\n",
    "        if value in added:\n",
    "            continue\n",
    "        groups.append((value, df[df[group_col] == value]))\n",
    "        added.add(value)\n",
    "    return groups\n",
    "\n",
    "\n",
    "def _add_category_rows(\n",
    "    rows: list[dict[str, str]],\n",
    "    groups: list[tuple[str, pd.DataFrame]],\n",
    "    metric_label: str,\n",
    "    column: str,\n",
    "    categories: Sequence[tuple[str, Sequence[str]]],\n",
    ") -> None:\n",
    "    for display_label, target_values in categories:\n",
    "        row = {\"Metric\": metric_label, \"Submetric\": display_label}\n",
    "        for group_name, group_df in groups:\n",
    "            row[group_name] = _format_count_pct(group_df.get(column), target_values)\n",
    "        rows.append(row)\n",
    "\n",
    "\n",
    "def build_baseline_table_by_group(df: pd.DataFrame) -> pd.DataFrame:\n",
    "    \"\"\"Return baseline characteristics table per patient group.\"\"\"\n",
    "    working = df.copy()\n",
    "    working.rename(columns=NAME_MAPPING, inplace=True)\n",
    "    working = normalise_string_categories(working)\n",
    "    working = translate_labels(working)\n",
    "\n",
    "    group_col = \"PATIENT GROUP\" if \"PATIENT GROUP\" in working else \"PATIENTENGROEP\"\n",
    "    groups = _build_group_sequence(working, group_col)\n",
    "    rows: list[dict[str, str]] = []\n",
    "\n",
    "    def add_simple_row(metric: str, func, submetric: str = \"\") -> None:\n",
    "        row = {\"Metric\": metric, \"Submetric\": submetric}\n",
    "        for group_name, group_df in groups:\n",
    "            row[group_name] = func(group_df)\n",
    "        rows.append(row)\n",
    "\n",
    "    add_simple_row(\"n\", lambda gdf: f\"{len(gdf)}\")\n",
    "    add_simple_row(\"Age (median [IQR])\", lambda gdf: _format_median_iqr(gdf.get(\"AGE\")))\n",
    "    add_simple_row(\n",
    "        \"Male sex (%)\",\n",
    "        lambda gdf: _format_count_pct(gdf.get(\"SEX\"), [\"male\", \"man\", \"m\"]),\n",
    "    )\n",
    "\n",
    "    _add_category_rows(rows, groups, \"Diabetes (%)\", \"DIABETES\", DIABETES_CATEGORY_LABELS)\n",
    "    _add_category_rows(rows, groups, \"Smoking (%)\", \"ROKEN\", SMOKING_CATEGORY_LABELS)\n",
    "\n",
    "    add_simple_row(\n",
    "        \"Systolic blood pressure (median [IQR])\",\n",
    "        lambda gdf: _format_median_iqr(gdf.get(\"SYS_BP\")),\n",
    "    )\n",
    "    add_simple_row(\n",
    "        \"Low-density lipoprotein cholesterol (median [IQR])\",\n",
    "        lambda gdf: _format_median_iqr(gdf.get(\"CHOLESTEROL_LDL\")),\n",
    "    )\n",
    "    add_simple_row(\n",
    "        \"Systematic Coronary Risk Evaluation score (median [IQR])\",\n",
    "        lambda gdf: _format_median_iqr(gdf.get(\"SCORE_2\")),\n",
    "    )\n",
    "    add_simple_row(\n",
    "        \"Blood pressure medication = No (%)\",\n",
    "        lambda gdf: _format_count_pct(gdf.get(\"BLOEDDRUK_MEDICATIE\"), [\"no\", \"nee\", \"0\"]),\n",
    "    )\n",
    "    add_simple_row(\n",
    "        \"Transient Ischaemic Attack = No (%)\",\n",
    "        lambda gdf: _format_count_pct(gdf.get(\"TIA\"), [\"no\", \"nee\", \"0\"]),\n",
    "    )\n",
    "\n",
    "    _add_category_rows(\n",
    "        rows,\n",
    "        groups,\n",
    "        \"Cerebrovascular Accident (%)\",\n",
    "        \"STROKE HISTORY\",\n",
    "        CVA_CATEGORY_LABELS,\n",
    "    )\n",
    "    _add_category_rows(\n",
    "        rows,\n",
    "        groups,\n",
    "        \"Patient group (%)\",\n",
    "        group_col,\n",
    "        PATIENT_GROUP_CATEGORY_LABELS,\n",
    "    )\n",
    "\n",
    "    table = pd.DataFrame(rows)\n",
    "    group_names = [name for name, _ in groups]\n",
    "    ordered_cols = [\"Metric\", \"Submetric\"] + group_names\n",
    "    return table.loc[:, ordered_cols]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ce117bb",
   "metadata": {},
   "source": [
    "## Run the preprocessing pipeline step by step\n",
    "\n",
    "From here on, we **run** the pipeline, approximating the original `preprocess(config)` function but in explicit notebook cells so you can inspect intermediate outputs.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e6f433",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 10 \u2013 Load raw data\n",
    "\n",
    "LOGGER.info(\"Loading raw tables from %s\", config.raw_dir)\n",
    "\n",
    "df_bl, meta_bl = read_sav(config.raw_dir / \"df.sav\")\n",
    "df_bl = apply_value_labels(df_bl, meta_bl)\n",
    "\n",
    "df_fu, meta_fu = read_sav(config.raw_dir / \"fu_2.sav\")\n",
    "df_fu = apply_value_labels(df_fu, meta_fu)\n",
    "\n",
    "cs_1 = pd.read_excel(config.raw_dir / \"compound_score.xlsx\")\n",
    "cs_2 = pd.read_csv(config.raw_dir / \"cs_cleaned_final.csv\", sep=\";\", decimal=\",\")\n",
    "\n",
    "for frame in (cs_1, cs_2):\n",
    "    if \"patientID\" in frame:\n",
    "        frame[\"patientID\"] = pd.to_numeric(frame[\"patientID\"], errors=\"coerce\")\n",
    "\n",
    "df_fu = df_fu.merge(cs_1, how=\"left\", on=\"patientID\").merge(cs_2, how=\"left\", on=\"patientID\")\n",
    "\n",
    "LOGGER.info(\"df_bl shape: %s\", df_bl.shape)\n",
    "LOGGER.info(\"df_fu shape (with compound scores): %s\", df_fu.shape)\n",
    "\n",
    "df_bl.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1545e34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 11 \u2013 Build outcomes and subset for BN variables\n",
    "\n",
    "outcomes, outcomes_sub = build_outcomes(df_fu)\n",
    "\n",
    "# Load codebook and filter layer information\n",
    "bn_vars = pd.read_excel(config.codebook_path, sheet_name=\"Items\")\n",
    "bn_vars.loc[bn_vars[\"VARIABLE NAME\"] == \"DROPOUT REASON\", \"LAYER\"] = \"L9 \u2013 Dropout\"\n",
    "\n",
    "subset, bn_vars_filter = prepare_subset(df_fu, bn_vars)\n",
    "\n",
    "LOGGER.info(\"Outcomes shape: %s\", outcomes.shape)\n",
    "LOGGER.info(\"Subset shape before merge: %s\", subset.shape)\n",
    "\n",
    "subset.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ddd726ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 12 \u2013 Merge outcomes into subset and harmonise columns\n",
    "\n",
    "df_final = subset.merge(outcomes_sub, how=\"left\", on=\"patientID\")\n",
    "\n",
    "harmonise_columns(df_final, bn_vars_filter)\n",
    "\n",
    "# Add extra layer rows and deduplicate\n",
    "extra = pd.DataFrame(EXTRA_LAYER_ROWS, columns=[\"LAYER\", \"VARIABLE NAME\"])\n",
    "extra.loc[extra[\"VARIABLE NAME\"] == \"DROPOUT REASON\", \"LAYER\"] = \"L9 \u2013 Dropout\"\n",
    "bn_vars_filter = pd.concat([bn_vars_filter, extra], ignore_index=True)\n",
    "bn_vars_filter.loc[bn_vars_filter[\"VARIABLE NAME\"] == \"DROPOUT REASON\", \"LAYER\"] = \"L9 \u2013 Dropout\"\n",
    "bn_vars_filter = bn_vars_filter.drop_duplicates(subset=[\"VARIABLE NAME\"])\n",
    "\n",
    "df_final = normalise_string_categories(df_final)\n",
    "\n",
    "# Convert labelled columns via simple string title-case for object columns\n",
    "df_final = df_final.apply(lambda col: col.astype(str).str.title() if col.dtype == object else col)\n",
    "\n",
    "LOGGER.info(\"df_final shape after harmonisation: %s\", df_final.shape)\n",
    "df_final.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30b0c082",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 13 \u2013 Compute SCORE2 and build reduced dataset\n",
    "\n",
    "df_clean_score = df_final.copy()\n",
    "\n",
    "sex_series = df_clean_score.get(\"SEX\")\n",
    "if sex_series is not None:\n",
    "    sex_lower = sex_series.astype(str).str.lower()\n",
    "else:\n",
    "    sex_lower = pd.Series(\"male\", index=df_clean_score.index)\n",
    "\n",
    "df_clean_score[\"SEX_SCORE\"] = np.where(sex_lower == \"female\", \"female\", \"male\")\n",
    "\n",
    "df_clean_score[\"ROKEN_SCORE\"] = np.where(\n",
    "    df_clean_score.get(\"ROKEN\", pd.Series(\"Nee\", index=df_clean_score.index)) == \"Ja\",\n",
    "    1,\n",
    "    0,\n",
    ")\n",
    "df_clean_score[\"DIABETES_SCORE\"] = np.where(\n",
    "    df_clean_score.get(\"DIABETES\", pd.Series(\"Ja\", index=df_clean_score.index)) == \"Nee\",\n",
    "    0,\n",
    "    1,\n",
    ")\n",
    "\n",
    "\n",
    "def row_score(row: pd.Series) -> float | None:\n",
    "    try:\n",
    "        return score2(\n",
    "            risk_region=config.risk_region,\n",
    "            age=float(row.get(\"AGE\")),\n",
    "            gender=row.get(\"SEX_SCORE\", \"male\"),\n",
    "            smoker=float(row.get(\"ROKEN_SCORE\", 0)),\n",
    "            systolic_bp=float(row.get(\"SYS_BP\")),\n",
    "            diabetes=float(row.get(\"DIABETES_SCORE\", 0)),\n",
    "            total_chol=float(row.get(\"CHOLESTEROL_TOTAAL\")),\n",
    "            total_hdl=float(row.get(\"CHOLESTEROL_HDL\")),\n",
    "        )\n",
    "    except (TypeError, ValueError):\n",
    "        return None\n",
    "\n",
    "\n",
    "df_clean_score[\"SCORE_2\"] = df_clean_score.apply(row_score, axis=1)\n",
    "\n",
    "# Reduced columns for BN\n",
    "reduced_cols = [\n",
    "    \"AGE\",\n",
    "    \"SEX\",\n",
    "    \"PTAU181\",\n",
    "    \"NFL\",\n",
    "    \"GFAP\",\n",
    "    \"AB40\",\n",
    "    \"AB42\",\n",
    "    \"MMSE_TOTAAL\",\n",
    "    \"STARKSTEIN\",\n",
    "    \"CDR\",\n",
    "    \"CVA\",\n",
    "    \"NEURORAD_SVD_SCORE\",\n",
    "    \"BCS_1\",\n",
    "    \"BCS_2\",\n",
    "    \"HV_ICV\",\n",
    "    \"TBV_ICV\",\n",
    "    \"CBF\",\n",
    "    \"MACE\",\n",
    "    \"OUTCOME_MACE\",\n",
    "    \"OUTCOME_CDR_INCREASE\",\n",
    "    \"T4_DROPOUT_REASON\",\n",
    "    \"SCORE_2\",\n",
    "    \"PATIENTENGROEP\",\n",
    "]\n",
    "df_clean_reduced = df_clean_score[[c for c in reduced_cols if c in df_clean_score]].copy()\n",
    "\n",
    "# Filter bn_vars for reduced set\n",
    "bn_vars_filter_2 = bn_vars_filter[bn_vars_filter[\"VARIABLE NAME\"].isin(df_clean_reduced.columns)].copy()\n",
    "bn_vars_filter_2[\"LAYER\"] = bn_vars_filter_2[\"LAYER\"].astype(str).str.strip()\n",
    "bn_vars_filter_2.loc[bn_vars_filter_2[\"VARIABLE NAME\"] == \"DROPOUT REASON\", \"LAYER\"] = \"L9 \u2013 Dropout\"\n",
    "bn_vars_filter_2 = bn_vars_filter_2.drop_duplicates(subset=[\"VARIABLE NAME\"])\n",
    "bn_vars_filter_2.loc[bn_vars_filter_2[\"VARIABLE NAME\"] == \"DROPOUT REASON\", \"LAYER\"] = \"L9 \u2013 Dropout\"\n",
    "\n",
    "# Apply external NAME_MAPPING\n",
    "df_clean_reduced.rename(columns=NAME_MAPPING, inplace=True)\n",
    "bn_vars_filter_2[\"VARIABLE NAME\"] = bn_vars_filter_2[\"VARIABLE NAME\"].replace(NAME_MAPPING)\n",
    "bn_vars_filter_2.loc[bn_vars_filter_2[\"VARIABLE NAME\"] == \"DROPOUT REASON\", \"LAYER\"] = \"L9 \u2013 Dropout\"\n",
    "\n",
    "df_clean_reduced = translate_labels(df_clean_reduced)\n",
    "df_clean_reduced = normalise_string_categories(df_clean_reduced)\n",
    "enforce_domain_categories(df_clean_reduced)\n",
    "\n",
    "LOGGER.info(\"df_clean_reduced shape: %s\", df_clean_reduced.shape)\n",
    "df_clean_reduced.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "744793ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 14 \u2013 Imputation and final outputs\n",
    "\n",
    "# Impute\n",
    "df_imp = impute_dataframe(df_clean_reduced, config.seed)\n",
    "enforce_domain_categories(df_imp)\n",
    "if \"ATHEROSCLEROTIC CARDIOVASCULAR DISEASE HISTORY\" in df_imp.columns:\n",
    "    df_imp[\"ATHEROSCLEROTIC CARDIOVASCULAR DISEASE HISTORY\"] = df_imp[\n",
    "        \"ATHEROSCLEROTIC CARDIOVASCULAR DISEASE HISTORY\"\n",
    "    ].astype(str)\n",
    "\n",
    "# Ensure output dir exists\n",
    "output_dir = config.output_dir\n",
    "output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Write parquet outputs\n",
    "df_clean_reduced.to_parquet(output_dir / \"df.parquet\", index=False)\n",
    "df_imp.to_parquet(output_dir / \"df_imp.parquet\", index=False)\n",
    "bn_vars_filter_2.to_parquet(output_dir / \"bn_vars.parquet\", index=False)\n",
    "\n",
    "LOGGER.info(\"Wrote df.parquet (%s rows)\", len(df_clean_reduced))\n",
    "LOGGER.info(\"Wrote df_imp.parquet (%s rows)\", len(df_imp))\n",
    "LOGGER.info(\"Wrote bn_vars.parquet (%s rows)\", len(bn_vars_filter_2))\n",
    "\n",
    "# Baseline table\n",
    "baseline_table = build_baseline_table_by_group(df_clean_score)\n",
    "if not baseline_table.empty:\n",
    "    baseline_csv = output_dir / \"baseline_table_by_group.csv\"\n",
    "    baseline_parquet = output_dir / \"baseline_table_by_group.parquet\"\n",
    "    baseline_table.to_csv(baseline_csv, index=False)\n",
    "    baseline_table.to_parquet(baseline_parquet, index=False)\n",
    "    LOGGER.info(\"Wrote baseline table (%s rows) to %s\", len(baseline_table), baseline_csv)\n",
    "else:\n",
    "    LOGGER.warning(\"Baseline table is empty; skipping export.\")\n",
    "\n",
    "baseline_table.head()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}