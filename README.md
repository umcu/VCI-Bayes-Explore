
# VCI-Bayes-Explore

![VCI-Bayes Logo](logo-vci-bayes.png)

Reproducible scripts for the Heart-Brain Connection Bayesian Network project by [Malin Overmars, PhD](https://github.com/loverma2) - [Vascular Cognitive Impairment (VCI) research group](https://research.umcutrecht.nl/research-groups/vascular-cognitive-impairment-vci/). 

For more information about the Heart-Brain Connection study, check: https://hart-brein.nl/ ü´Äüß†. 

The accompanying manuscript is currently in preparation üìÑ

## License & Citation

This repository is released under the [MIT License](LICENSE).

If you use this code, please cite: [![DOI](https://zenodo.org/badge/1067978388.svg)](https://doi.org/10.5281/zenodo.17302710). Thanks!

---

> ‚ö†Ô∏è **Important:** This code is tailored to the Heart-Brain Connection cohort and its definitions. If you apply it to a different dataset, review and adapt both the preprocessing (`src/preprocess_data.py`) and the Bayesian-network notebook (`src/bayesian_network.ipynb`) so the logic matches your cohort‚Äôs structure, coding, and requirements.

## Quick start

1. **Install Python 3.13+** (e.g. from [python.org](https://www.python.org/downloads/)) and open a terminal in the project folder.
2. **Create a virtual environment (optional but recommended):**
   ```bash
   python -m venv venv
   source venv/bin/activate # Windows: venv\Scripts\activate
   pip install pandas numpy pyreadstat scikit-learn PyYAML pyagrum matplotlib scipy ipywidgets
   ```
3. **Tell the pipeline where your raw data live:**
   ```bash
   cp config/data_paths.example.yml config/data_paths.yml
   ```
   Edit `config/data_paths.yml` and fill in:
   - `raw_dir`: folder with the SPSS `.sav` files
   - `codebook_path`: path to `HBC_CODEBOOK_LABELS.xlsx`
   - `output_dir`: leave as `src/out` to keep processed data inside the repo
4. **Preprocess the data:**
   ```bash
   python src/preprocess_data.py
   ```
   You should see log messages ending with ‚ÄúWrote df.parquet ‚Ä¶‚Äù. The processed files (`df.parquet`, `df_imp.parquet`, `bn_vars.parquet`) appear in `src/out/`.
5. **Open the analysis notebook:** launch Jupyter and run `src/bayesian_network.ipynb`. The first configuration cell automatically reads the parquet files from `src/out/`. Click ‚ÄúRun All‚Äù to reproduce the figures.

That‚Äôs it‚Äîyou now have the same dataset and model that produced the manuscript figures.  Need more control? Jump to the sections below.

---

## Data preparation in detail

### 1. Configure file locations

`config/data_paths.yml` keeps sensitive paths out of version control (the file is git-ignored). It accepts the following keys:

| Key | Description |
| --- | --- |
| `raw_dir` | Folder containing the raw SPSS exports (`df.sav`, `fu_2.sav`, etc.). |
| `codebook_path` | Absolute path to `HBC_CODEBOOK_LABELS.xlsx`. |
| `output_dir` | Where processed parquet files are written. Default behaviour writes to `src/out`. |
| `risk_region` | SCORE2 region used for cardiovascular risk (defaults to `"Low"`). |

All paths may be relative to the repository root. Example:

```yaml
risk_region: Low
raw_dir: "/secure/location/hartbrein/raw"
codebook_path: "/secure/location/hartbrein/meta/HBC_CODEBOOK_LABELS.xlsx"
output_dir: "src/out"
```

### 2. Run the preprocessing script

```bash
python src/preprocess_data.py
```

The script:

* reads the raw SPSS tables and codebook,
* applies the SPSS value labels (Dutch ‚Üí English),
* constructs the outcome variables (`OUTCOME_MACE`, `OUTCOME_CDR_INCREASE`)
* computes the SCORE2 cardiovascular risk score,
* imputes missing numeric values with `IterativeImputer`,
* writes `df.parquet`, `df_imp.parquet`, and `bn_vars.parquet` to `src/out/`.
---

## Running the Bayesian-network notebook

1. Start Jupyter (or VS Code, or JupyterLab) in the repository.
2. Open `src/bayesian_network.ipynb`.
3. Execute the cells in order. The first cell auto-detects the processed data under `src/out/` and loads:
   - `df.parquet`: the labelled, non-imputed dataset (categorical labels preserved).
   - `df_imp.parquet`: the imputed dataset used for learning.
   - `bn_vars.parquet`: metadata linking each variable to its expert-defined layer.
4. Subsequent sections:
   - **Discretisation** ‚Äî uses `pyAgrum`‚Äôs `DiscreteTypeProcessor` with quantile-based binning.
   - **Structure learning** ‚Äî enforces the layer constraints and adds explicit arcs from the outcome nodes to the dropout layer.
   - **Inference & visualisation** ‚Äî produces network and posterior plots, CPT displays
---

## Repository layout

```
‚îú‚îÄ‚îÄ README.md                 Project guide (this file)
‚îú‚îÄ‚îÄ logo-vci-bayes.png        Banner used in the README
‚îú‚îÄ‚îÄ config/
‚îÇ   ‚îú‚îÄ‚îÄ data_paths.example.yml Template pointing to raw data
‚îÇ   ‚îî‚îÄ‚îÄ config.yaml           Extra notebook settings (optional)
‚îú‚îÄ‚îÄ src/
‚îÇ   ‚îú‚îÄ‚îÄ preprocess_data.py    End-to-end data preparation script
‚îÇ   ‚îú‚îÄ‚îÄ bayesian_network.ipynb Main analysis and figures
‚îÇ   ‚îî‚îÄ‚îÄ out/                  Default location for processed parquet files
‚îî‚îÄ‚îÄ docs/, graphs/, cache/, ‚Ä¶ Supporting material
```
---

## Requirements

- Python ‚â• 3.12
- pandas, numpy
- pyreadstat
- scikit-learn
- PyYAML
- matplotlib
- PyAgrum (including `pyagrum.skbn`, `pyagrum.lib.notebook`, etc.)
- SciPy
- ipywidgets

Install manually (`pip install ‚Ä¶`) or via a requirements file if you maintain one.

